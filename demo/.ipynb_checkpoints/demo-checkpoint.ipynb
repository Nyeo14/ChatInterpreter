{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitch Chat Interpreter, demo \n",
    "### members: \n",
    "##### Nate Yeo, 42099639, yeon@uci.edu\n",
    "##### Jingtian Li, 46918353, jingtil2@uci.edu\n",
    "##### Duo Chen, 92789693, duoc3@uci.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the demonstration for part of our twitch chat interpreter project. This demo will be a streamline of what we have accomplished. For time saving, the demonstrated part will only be what we feel worth showing. Outputs of parts not included are presented as screen snippets in the project report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Purpose & First part\n",
    "We aim to make a system that can automically clip from a stream vod by using chat, thus saving editors' time in finding potential clips. Selected stream vods are downloaded by using TwitchDownloader (third party software), files are in the project folder named \"chatjsonfiles\". \n",
    "\n",
    "Our first approach was to gather stream clips based on chat speed. This is done using the Clipper.py module, whose terminal outputs are in report's screen snippets and whose files outputs are in the project folder \"clip_data\". \n",
    "\n",
    "We want to train a learner to distinguish between good/bad clips from a collection of clips. To do this, we used labeler to manually label those clips. Terminal outputs are in project report and file outputs are in project folder \"labeled_clip_data\". \n",
    "\n",
    "To numericalize string texts, we made a Embedding module by using gensim python package. We then trained it on the json files we have. Here is part of its results. NOTE: to run the cell for live demo of Embedding (second next), you will need the gensim package, numpy, scipy and nltk package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrating [3] chosen words and 5 of their most similar words: \n",
      "For word [lol]\n",
      ">> [lulw]:[0.9854356050491333]\n",
      ">> [fuck]:[0.9765660762786865]\n",
      ">> [haha]:[0.974907398223877]\n",
      ">> [omegalul]:[0.9736191630363464]\n",
      ">> [lmao]:[0.9641276597976685]\n",
      "\n",
      "For word [haha]\n",
      ">> [lmao]:[0.9905922412872314]\n",
      ">> [lulw]:[0.9809660315513611]\n",
      ">> [omegalul]:[0.9793339967727661]\n",
      ">> [lol]:[0.974907398223877]\n",
      ">> [kekw]:[0.9713457822799683]\n",
      "\n",
      "For word [pog]\n",
      ">> [pogu]:[0.9807208180427551]\n",
      ">> [pogcrew]:[0.9568219780921936]\n",
      ">> [katielpog]:[0.9555957317352295]\n",
      ">> [clutch]:[0.9483774900436401]\n",
      ">> [kreygasm]:[0.9365337491035461]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For this cell, you only need the python pickle package \n",
    "# This cell demonstrates three words we have chosen to present their similar words and consine similarity \n",
    "# vector size is 300 (too big to show), thus we will not show vectors in this cell output\n",
    "import pickle \n",
    "with open('data/EmbdDemo.pkl','rb') as f: \n",
    "    demo=pickle.load(f) \n",
    "    \n",
    "print(f\"Demonstrating [{len(demo)}] chosen words and 5 of their most similar words: \") \n",
    "for word in demo.keys(): \n",
    "    print(f\"For word [{word}]\") \n",
    "    for w in demo[word]: print(f\">> [{w[0]}]:[{w[1]}]\")\n",
    "    print()\n",
    "\n",
    "# Somethings about those words: \n",
    "# lulw, omegalul, lmao, etc are variants of the word lol commonly used on Twitch \n",
    "# kekw is a Twitch emote, if you want to know what it looks like, google image \"kekw\" \n",
    "# pog is an emote on Twitch to express amazement \n",
    "# pog has variants such as pogu, pogcrew etc.\n",
    "# Many of these words are Twitch emotes, feel free to look them up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this cell, you will need the gensim python package and many other packages specified above\n",
    "# we want to demonstrate the complete w2v model we have, including some functions in Embedding for better readibility \n",
    "# How to use the terminal UI is somewhat included in itself, feel free to play around with it. \n",
    "# If you want to save time in not installing gensim, just skip this cell \n",
    "import Embedding \n",
    "kv=Embedding.Load_wv('data/teo.kv') \n",
    "Embedding.Check_trained_model(kv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Vocabulary size is: [1804]\n",
      "Enter either one word to find most similar or two to find similarity, enter 0 to exit\n",
      "Enter here: lol\n",
      "How many similar words do you want to see?: 5\n",
      "Do you want to see the vector? (y/n): n\n",
      "---------------------------------------------------------------\n",
      "5 most similar words of [lol] are: \n",
      ">>[lulw]: 0.9854356050491333 \n",
      ">>[fuck]: 0.9765660762786865 \n",
      ">>[haha]: 0.974907398223877 \n",
      ">>[omegalul]: 0.9736191630363464 \n",
      ">>[lmao]: 0.9641276597976685 \n",
      "\n",
      "---------------------------------------------------------------\n",
      "Vocabulary size is: [1804]\n",
      "Enter either one word to find most similar or two to find similarity, enter 0 to exit\n",
      "Enter here: haha\n",
      "How many similar words do you want to see?: 3\n",
      "Do you want to see the vector? (y/n): n\n",
      "---------------------------------------------------------------\n",
      "3 most similar words of [haha] are: \n",
      ">>[lmao]: 0.9905922412872314 \n",
      ">>[lulw]: 0.9809660315513611 \n",
      ">>[omegalul]: 0.9793339967727661 \n",
      "\n",
      "---------------------------------------------------------------\n",
      "Vocabulary size is: [1804]\n",
      "Enter either one word to find most similar or two to find similarity, enter 0 to exit\n",
      "Enter here: lol haha\n",
      "---------------------------------------------------------------\n",
      "[lol]:[haha] has similarity 0.9749072790145874\n",
      "---------------------------------------------------------------\n",
      "Vocabulary size is: [1804]\n",
      "Enter either one word to find most similar or two to find similarity, enter 0 to exit\n",
      "Enter here: 0\n"
     ]
    }
   ],
   "source": [
    "# This cell demonstrate some word purification processes done in LogReg\n",
    "cmd = input(\"what do you want to test? 1 -> simple purification; 2 -> heavy purification; enter other to quit\")\n",
    "while (cmd in [\"1\", \"2\"]):\n",
    "    be_converted = input(\"please enter the string you wish to convert\")\n",
    "    print(log_regression.demo_convert(be_converted, cmd))\n",
    "    cmd = input(\"what do you want to test? 1 -> simple purification; 2 -> heavy purification; enter other to quit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second part\n",
    "\n",
    "Models we have tried are LogReg, MLP, RNN, GRU. Because we implemented those python modules to train AND test models in one go, we do not have enough time to re-write codes for this demonstration. But the typical outputs are included in project report.\n",
    "\n",
    "We found that learner models does not have significant increase compared to default accuracy, and that many of our clips made from chat speed are noisy in terms of chat content. \n",
    "\n",
    "This lead to the making of ClipperV2.py. In this approach, we identify clips by locating intervals of chats whose tokens have similar vector directions. Ideally, this should find clips where chats have homogeneous reactions. \n",
    "\n",
    "Because it was close to the end of the quarter, we did not make a terminal UI based python model. Thus running this py file will directly output terminal results. The ClipperV2 used in demo is different from the one in project, thus the only additional packages needed for this part should be nltk, numpy and scipy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing data\n",
      "clipping vod\n",
      "clip for file [data/teo6.pkl]\n",
      "number of clips found: [54]\n",
      "Clip number [1] is: [0:7:34] -> [0:8:50]\n",
      "Clip number [3] is: [0:19:34] -> [0:20:32]\n",
      "Clip number [4] is: [0:20:37] -> [0:21:35]\n",
      "Clip number [5] is: [0:23:52] -> [0:24:50]\n",
      "Clip number [6] is: [0:30:4] -> [0:31:2]\n",
      "Clip number [7] is: [0:38:31] -> [0:40:2]\n",
      "Clip number [8] is: [0:43:31] -> [0:44:29]\n",
      "Clip number [9] is: [0:45:13] -> [0:46:11]\n",
      "Clip number [10] is: [0:48:34] -> [0:50:5]\n",
      "Clip number [11] is: [0:52:43] -> [0:54:17]\n",
      "Clip number [12] is: [0:57:19] -> [0:58:17]\n",
      "Clip number [13] is: [1:10:22] -> [1:11:29]\n",
      "Clip number [14] is: [1:14:1] -> [1:15:23]\n",
      "Clip number [15] is: [1:16:1] -> [1:17:38]\n",
      "Clip number [16] is: [1:21:46] -> [1:23:8]\n",
      "Clip number [17] is: [1:23:43] -> [1:24:41]\n",
      "Clip number [18] is: [1:26:10] -> [1:27:26]\n",
      "Clip number [19] is: [1:29:34] -> [1:30:41]\n",
      "Clip number [20] is: [1:37:25] -> [1:39:29]\n",
      "Clip number [21] is: [1:43:37] -> [1:45:2]\n",
      "Clip number [22] is: [1:48:4] -> [1:50:23]\n",
      "Clip number [23] is: [2:0:28] -> [2:2:20]\n",
      "Clip number [24] is: [2:5:55] -> [2:6:53]\n",
      "Clip number [25] is: [2:17:22] -> [2:18:20]\n",
      "Clip number [27] is: [2:34:52] -> [2:36:2]\n",
      "Clip number [28] is: [2:36:40] -> [2:37:38]\n",
      "Clip number [29] is: [2:38:1] -> [2:38:59]\n",
      "Clip number [31] is: [2:50:13] -> [2:52:23]\n",
      "Clip number [32] is: [2:53:46] -> [2:54:44]\n",
      "Clip number [33] is: [2:56:4] -> [2:57:2]\n",
      "Clip number [34] is: [2:57:22] -> [2:58:20]\n",
      "Clip number [35] is: [2:59:10] -> [3:0:8]\n",
      "Clip number [36] is: [3:0:22] -> [3:1:20]\n",
      "Clip number [37] is: [3:4:10] -> [3:5:8]\n",
      "Clip number [38] is: [3:10:55] -> [3:11:56]\n",
      "Clip number [39] is: [3:12:10] -> [3:13:11]\n",
      "Clip number [40] is: [3:18:19] -> [3:19:17]\n",
      "Clip number [41] is: [3:20:16] -> [3:21:38]\n",
      "Clip number [42] is: [3:22:40] -> [3:23:38]\n",
      "Clip number [43] is: [3:26:58] -> [3:27:56]\n",
      "Clip number [44] is: [3:31:16] -> [3:32:14]\n",
      "Clip number [45] is: [3:49:55] -> [3:51:44]\n",
      "Clip number [46] is: [4:6:13] -> [4:7:11]\n",
      "Clip number [47] is: [4:11:4] -> [4:12:2]\n",
      "Clip number [48] is: [4:18:46] -> [4:21:29]\n",
      "Clip number [49] is: [4:22:52] -> [4:23:50]\n",
      "Clip number [50] is: [4:27:25] -> [4:28:32]\n",
      "Clip number [51] is: [4:29:43] -> [4:30:47]\n",
      "Clip number [53] is: [4:42:1] -> [4:43:35]\n",
      "total time to edit is [1:0:34]\n",
      "original vod duration is [4:48:42]\n"
     ]
    }
   ],
   "source": [
    "%run ClipperV2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings back the project purpose, which is pre-processing stream vods into collection of clips. As you can see, a vod of almost 5 hours can be shrunken down to 1 hour (whose content we have checked) in unsupervised clipping. Hence, this approach is viable. \n",
    "\n",
    "The vod URL is https://www.twitch.tv/videos/866021945 However, Twitch delete out-dated vod to save storage. This vod is likely deleted by the time you are reading this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
