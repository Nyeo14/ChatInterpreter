{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\wizard king\n",
      "[nltk_data]     rabbit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a path to a file or a folder to add that to the training set, enter e to exitlabeled_clip_data/wardell/wardell[0]_labeled.pkl\n",
      "enter a path to a file or a folder to add that to the training set, enter e to exitlabeled_clip_data/wardell/wardell[1]_labeled.pkl\n",
      "enter a path to a file or a folder to add that to the training set, enter e to exite\n",
      "What proportion of the training data would be used for validation?0.2\n",
      "Do you want to use default english stopwords or stopwords given by my author? (default/author)default\n",
      "enter y to look at top 5 significant terms, enter other to quity\n",
      "Positive weight rank  1 : \n",
      "---> kew , and its weight is:  0.01890215228547355\n",
      "Positive weight rank  2 : \n",
      "---> com , and its weight is:  0.00968700759126041\n",
      "Positive weight rank  3 : \n",
      "---> pod , and its weight is:  0.007478181989430052\n",
      "Positive weight rank  4 : \n",
      "---> pod com , and its weight is:  0.006732046783943352\n",
      "Positive weight rank  5 : \n",
      "---> number_word , and its weight is:  0.0065689653631929275\n",
      "Negative weight rank  1 : \n",
      "---> subrogate , and its weight is:  -0.005317390125574802\n",
      "Negative weight rank  2 : \n",
      "---> raid , and its weight is:  -0.004875685975205852\n",
      "Negative weight rank  3 : \n",
      "---> subrogate subrogate , and its weight is:  -0.00448828844253621\n",
      "Negative weight rank  4 : \n",
      "---> rota , and its weight is:  -0.00333494954530307\n",
      "Negative weight rank  5 : \n",
      "---> rota raid , and its weight is:  -0.002998997570722027\n",
      "Number of training examples: [34]\n",
      "Vocabulary size: [7014]\n",
      "Training accuracy: [91.18]\n",
      "Training AUC value: [97.93]\n",
      " default accuracy: [14.71]\n",
      "\n",
      "Validation/Testing accuracy: [66.67]\n",
      "Validation/Testing AUC value: [66.67]\n",
      " default accuracy: [33.33]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "import simplejson as json\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import * \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from Utilities import *\n",
    "from Tokenizer_kit import *\n",
    "from Embedding import *\n",
    "from Data_loader import *\n",
    "from Data_converter import *\n",
    "from random import shuffle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import enchant\n",
    "global_dict = enchant.Dict(\"en_US\")\n",
    "global_slang = set({\"F\", \"???\", \"!!!\", \"!?\", \"pog\", \"nice\", \"noice\", \"haha\", \"lol\", \"lul\", \"lmao\", \"yes\", \"noo\", \"no\", \"yeah\", \"ree\", \"oof\", \"pogu\", \"xd\", \"ez\", \"money\", \"GG\", \"gg\"})\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# nltk.download('stopwords') is needed\n",
    "# the following 2 functions are from HW1, with some modification.\n",
    "def logistic_classification(X, Y, classifier = None):\n",
    "\tmsg_line = \"\"\n",
    "\tif (classifier == None):\n",
    "\t\tmode = \"Training\"\n",
    "\t\tmsg_line += f\"Number of training examples: [{X.shape[0]}]\" + os.linesep\n",
    "\t\tmsg_line += f\"Vocabulary size: [{X.shape[1]}]\" + os.linesep\n",
    "\t\tclassifier = linear_model.LogisticRegression(penalty = 'l2', tol = 0.3, solver = \"sag\", max_iter = 10)\n",
    "\t\tclassifier.fit(X, Y)\n",
    "\telse:\n",
    "\t\tmode = \"Validation/Testing\"\n",
    "\taccuracy = classifier.score(X, Y)\n",
    "\tmsg_line += mode + f\" accuracy: [{format( 100*accuracy , '.2f')}]\" + os.linesep\n",
    "\ttrain_predictions = classifier.predict(X)\n",
    "\tclass_probabilities = classifier.predict_proba(X)\n",
    "\ttest_auc_score = sklearn.metrics.roc_auc_score(Y, class_probabilities[:,1])\n",
    "\tmsg_line += mode + f\" AUC value: [{format( 100*test_auc_score , '.2f')}]\" + os.linesep\n",
    "\tdefault_counter = 0\n",
    "\tfor i in Y:\n",
    "\t\tif (i == 0):\n",
    "\t\t\tdefault_counter += 1\n",
    "\tdefault_accuracy = default_counter / len(Y)\n",
    "\tmsg_line += f\" default accuracy: [{format( 100*default_accuracy , '.2f')}]\" + os.linesep\n",
    "\tcounter = 0\n",
    "\tmy_error = []\n",
    "\twhile (counter < X.shape[0]):\n",
    "\t\tif (train_predictions[counter] != Y[counter]):\n",
    "\t\t\tmy_error.append(counter)\n",
    "\t\tcounter += 1\n",
    "\treturn classifier, my_error, msg_line\n",
    "\n",
    "def most_significant_terms(classifier, vectorizer, K):\n",
    "\tcount = 0\n",
    "\ttopK_pos_weights = set()\n",
    "\ttopK_pos_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmax = -1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight > 0 and weight > max and not weight in topK_pos_weights):\n",
    "\t\t\t\tmax = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not max == -1):\n",
    "\t\t\ttopK_pos_weights.add(max)\n",
    "\t\t\ttopK_pos_terms.add(temp_term)\n",
    "\t\t\tprint(\"Positive weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(max))\n",
    "\t\tcount += 1\n",
    "\tcount = 0\n",
    "\ttopK_neg_weights = set()\n",
    "\ttopK_neg_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmin = 1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight < 0 and weight < min and not weight in topK_neg_weights):\n",
    "\t\t\t\tmin = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not min == 1):\n",
    "\t\t\ttopK_neg_weights.add(min)\n",
    "\t\t\ttopK_neg_terms.add(temp_term)\n",
    "\t\t\tprint(\"Negative weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(min))\n",
    "\t\tcount += 1\n",
    "\treturn(topK_pos_weights, topK_neg_weights, topK_pos_terms, topK_neg_terms)\n",
    "\n",
    "# directly convert a list of long strings into a one-hot vector\n",
    "# it does both tokenization and vectorization\n",
    "# it should returns an 2-D array\n",
    "# X index the clip, Y index the token\n",
    "def to_ohv(text_list, stop_words = [], min_len = 2):\n",
    "\ttoken_set = set()\n",
    "\tfor text in text_list:\n",
    "\t\tfor word in text.split():\n",
    "\t\t\tif (len(word) > min_len) and (not word in stop_words) and (not word in set(stopwords.words('english'))) and (not word in token_set):\n",
    "\t\t\t\ttoken_set.add(word)\n",
    "\tresult = np.zeros((len(text_list), len(token_set)))\n",
    "\tfor i, token in enumerate(token_set):\n",
    "\t\tfor ii, sentence in enumerate(text_list):\n",
    "\t\t\tif (token in sentence):\n",
    "\t\t\t\tresult[ii][i] = 1\n",
    "\treturn result\n",
    "\n",
    "# translate a word into something standard\n",
    "def my_translator(target_word, stop_words = nltk_stop_words):\n",
    "\tresult = target_word\n",
    "\tif global_dict.check(target_word):\n",
    "\t\t# this word is a standard word, return it\n",
    "\t\tresult =  target_word\n",
    "\telif (target_word in global_slang):\n",
    "\t\t# this word is not a standard word, is it an internet slang?\n",
    "\t\tresult =  target_word\n",
    "\telif (Embedding_word_modifier(target_word) in global_slang):\n",
    "\t\t# Or it could be some special form of an iternet slang\n",
    "\t\tresult =  Embedding_word_modifier(target_word)\n",
    "\telif len(global_dict.suggest(target_word)) > 0:\n",
    "\t\t# it is nothing but there are similar words\n",
    "\t\tresult =  global_dict.suggest(target_word)[0]\n",
    "\telse:\n",
    "\t\t# it is nothing, probably an emote\n",
    "\t\t# but we do not have a similar word to it, so return itself\n",
    "\t\tresult = target_word\n",
    "\ttry:\n",
    "\t\t# is it a number? Maybe we should purify numbers\n",
    "\t\t_test = float(target_word)\n",
    "\t\tresult = \"NUMBER_WORD\"\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tif result == None:\n",
    "\t\tresult =  target_word\n",
    "#\telif result in stop_words:\n",
    "#\t\t# if the result in in stop_words, we should not bother returning it\n",
    "#\t\tpass\n",
    "\telse:\n",
    "\t\tresult =  result\n",
    "#\treturn result\n",
    "\t# last step : remove redundant consequtive words\n",
    "\treal_result = []\n",
    "\tlast_letter = None\n",
    "\tfor letter in result:\n",
    "\t\tif (not last_letter == None) and (letter == last_letter):\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\treal_result.append(letter)\n",
    "\t\tlast_letter = letter\n",
    "\treturn Concatenate_str_list(real_result, splitter = '')\n",
    "        \n",
    "# please be sure that you give it a valid path when using it\n",
    "def add_filepath_to_set(the_path:str, is_file:bool, original_set):\n",
    "    if (is_file):\n",
    "        original_set.append(the_path)\n",
    "    else:\n",
    "        for entry in os.scandir(the_path):\n",
    "            if (entry.path.endswith(\".pkl\") and entry.is_file()):\n",
    "                original_set.append(entry.path)\n",
    "    return original_set\n",
    "\n",
    "# interpret a pkl file and extract its data into three lists\n",
    "def add_clipdata_to_set(clip_list, text_list, y_list, pkl_path, do_convert = True, filter_stopword = True, show_debug = True):\n",
    "\tthe_file = open(pkl_path, 'rb')\n",
    "\tthe_pkl = pickle.load(the_file)\n",
    "\tfor clip in the_pkl:\n",
    "\t\tclip_list.append(clip)\n",
    "\t\tif do_convert:\n",
    "\t\t\ttemp_text = []\n",
    "\t\t\tfor chat in clip.chats:\n",
    "\t\t\t\tfor word in chat.split():\n",
    "\t\t\t\t\ttemp_word = my_translator(word)\n",
    "\t\t\t\t\ttemp_text.append(temp_word)\n",
    "\t\t\tif (show_debug):\n",
    "\t\t\t\tprint(temp_text)\n",
    "\t\t\ttext_list.append(Concatenate_str_list(temp_text))\n",
    "\t\telse:\n",
    "\t\t\ttext_list.append(Concatenate_str_list(clip.chats))\n",
    "\t\tif (clip.get_label_binary() == 0):\n",
    "\t\t\ty_list.append(0)\n",
    "\t\telse:\n",
    "\t\t\ty_list.append(1)\n",
    "\tthe_file.close()\n",
    "\treturn clip_list, text_list, y_list\n",
    "\n",
    "# randomize data\n",
    "def randomize_data(clip_list, text_list, y_list):\n",
    "\torder_list = list(range(len(clip_list)))\n",
    "\trandom.shuffle(order_list)\n",
    "\tnew_clip = list(clip_list)\n",
    "\tnew_text = list(text_list)\n",
    "\tnew_y = list(y_list)\n",
    "\tfor i, ii in enumerate(order_list):\n",
    "\t\tnew_clip[i] = clip_list[ii]\n",
    "\t\tnew_text[i] = text_list[ii]\n",
    "\t\tnew_y[i] = y_list[ii]\n",
    "\treturn new_clip, new_text, new_y\n",
    "\n",
    "# this function iteratively run the main to find the best param\n",
    "def best_param(ngram, panelty, dual, tol, C, fit_intercept, solver, max_iter, num_iter = 10, test_ratio = 0.2, test_on = [\"labeled_clip_data/Teo\", \"labeled_clip_data/wardell\", \"labeled_clip_data/T90\"]):\n",
    "\tva_err_list = []\n",
    "\t#define training set\n",
    "\tfilepath = []\n",
    "\ttext = []\n",
    "\tY = []\n",
    "\tall_clip = []\n",
    "\tfor path in test_on:\n",
    "\t\tfilepath = add_filepath_to_set(path, False, filepath)\n",
    "\tfor filename in filepath:\n",
    "\t\tall_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "\t#define validation set\n",
    "\ttraining_size = int(len(Y) * (1 - test_ratio))\n",
    "\tvalidation_size = len(Y) - training_size\n",
    "\t# iteratively test the model\n",
    "\twhile (num_iter > 0):\n",
    "\t\t# randomize the data\n",
    "\t\tall_clip, text, Y = randomize_data(all_clip, text, Y)\n",
    "\t\t# construct the vectorizer\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, ngram), stop_words = 'english', min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\t\tX = vect.fit_transform(text)\n",
    "\t\t# make classifier\n",
    "\t\t# the following line is responsible for taking different parameters\n",
    "\t\tclassifier = linear_model.LogisticRegression(C = C, dual = dual, penalty = panelty, fit_intercept = fit_intercept, tol = tol, solver = solver, max_iter = max_iter)\n",
    "\t\tclassifier.fit(X[:training_size], Y[:training_size])\n",
    "\t\tva_err_list.append(classifier.score(X[training_size:], Y[training_size:]))\n",
    "\t\tnum_iter -= 1\n",
    "\treturn np.average(va_err_list)\n",
    "\n",
    "def sudo_main(ask_save = True, ask_test = True, if_debug = True): \n",
    "    # main function, a sequence of supportive methods defined above \n",
    "    # see specifications in learner_output.txt \\\n",
    "    # one good practice is to keep indent within a function no more than 3\n",
    "    # if more loop like structures are needed, another defined method is recommended\n",
    "\n",
    "    #define training set\n",
    "    text = []\n",
    "    Y = []\n",
    "    all_clip = []\n",
    "    filepath = []\n",
    "    file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "    while(file_or_folder != \"e\"):\n",
    "        filepath = add_filepath_to_set(file_or_folder, _type == \"file\", filepath)\n",
    "        file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "    for filename in filepath:\n",
    "        all_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "    #define validation set\n",
    "    validation_ratio = prompt_for_float(\"What proportion of the training data would be used for validation?\", 0, 1)\n",
    "    training_size = int(len(Y) * (1 - validation_ratio))\n",
    "    validation_size = len(Y) - training_size\n",
    "    # randomize the data\n",
    "    all_clip, text, Y = randomize_data(all_clip, text, Y)\n",
    "    # train the model\n",
    "    classifier, t_err, v_err, t_msg, v_msg = main(text, Y, training_size, validation_size)\n",
    "    if if_debug:\n",
    "        print(t_msg)\n",
    "        print(v_msg)\n",
    "    #save the mislabeled\n",
    "    if (ask_save and prompt_for_str(\"Do you want to save the mislabeled clips? (y/n) \") == \"y\"):\n",
    "        if not os.path.isdir(\"/mislabeled\"):\n",
    "            os.mkdir(\"/mislabeled\")\n",
    "        file_prefix = prompt_for_str(\"Please name the prefix of saved files: \")\n",
    "        # making mislabeled file for training errors\n",
    "        err_list = list()\n",
    "        for err_id in t_err:\n",
    "            err_list.append(all_clip[err_id])\n",
    "        new_file_path = 'mislabeled/' + file_prefix + '_mislabeled_train.pkl' \n",
    "        with open(new_file_path, 'wb') as f: \n",
    "            pickle.dump(err_list, f)\n",
    "        # making mislabeled file for validation errors\n",
    "        err_list = list()\n",
    "        for err_id in v_err:\n",
    "            err_list.append(all_clip[err_id + training_size])\n",
    "        new_file_path = 'mislabeled/' + file_prefix + '_mislabeled_validation.pkl' \n",
    "        with open(new_file_path, 'wb') as f: \n",
    "            pickle.dump(err_list, f)\n",
    "    # test the classifier\n",
    "    training_size = len(Y)\n",
    "    while (ask_test and input(\"Do you want to test this classifier on any unlabled clip data? (y/n)\") == \"y\"):\n",
    "        all_clip = []\n",
    "        training_size = len(Y)\n",
    "        file_path = prompt_for_file(\"which file you want to do test on? \")\n",
    "        if_answer = input(\"Is this file labeled? (y/n)\") == \"y\"\n",
    "        all_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, file_path)\n",
    "        classifier, t_err, v_err, t_msg, v_msg = main(text, Y, training_size, len(Y) - training_size, if_answer)\n",
    "        if if_answer:\n",
    "            print(v_msg)\n",
    "        else:\n",
    "            counter = 0\n",
    "            while(counter < len(all_clip)):\n",
    "                all_clip[counter].labeled = v_msg[counter]\n",
    "                counter += 1\n",
    "            file_path = prompt_for_save_file(dir_path='model_labeled_result', f_format='.pkl')\n",
    "            with open(file_path, 'wb') as f: \n",
    "                pickle.dump(all_clip, f)\n",
    "    return classifier.score(text[:training_size], Y[:training_size]), classifier.score(text[training_size:], Y[training_size:])\n",
    "\n",
    "# main\n",
    "def main(the_text = None, the_y = None, t_size = None, v_size = None, test_has_answer = True, always_default = False):\n",
    "\tif (the_text == None):\n",
    "\t\tsudo_main()\n",
    "\t\treturn\n",
    "\t# define stop word\n",
    "\tif (not always_default):\n",
    "\t\tif_stop = prompt_for_str(\"Do you want to use default english stopwords or stopwords given by my author? (default/author)\", {\"default\",\"author\"})\n",
    "\t\tif (if_stop == \"default\"):\n",
    "\t\t\tspecial_stop_word = set(stopwords.words('english'))\n",
    "\t\tif (if_stop == \"author\"):\n",
    "\t\t\tspecial_stop_word = {\"1\", \"2\", \"11\", \"111111\", \"gg\", \"gg gg\", \"LUL\", \"LOL\"}\n",
    "\t# construct the vectorizer\n",
    "\tif (special_stop_word == None):\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, 2), stop_words = 'english', min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\telse:\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, 2), stop_words = special_stop_word, min_df = 0.01,  tokenizer = Embedding_tokenize)\n",
    "\tX = vect.fit_transform(the_text)\n",
    "\t#X = to_ohv(the_text)\n",
    "\t# make classifier\n",
    "\tclassifier, t_err, t_msg = logistic_classification(X[:t_size], the_y[:t_size])\n",
    "\tif test_has_answer:\n",
    "\t\t_c, v_err, v_msg = logistic_classification(X[t_size:], the_y[t_size:], classifier)\n",
    "\t# look at result\n",
    "\tif ((not always_default) and input(\"enter y to look at top 5 significant terms, enter other to quit\") == \"y\"):\n",
    "\t\tmost_significant_terms(classifier, vect, 5)\n",
    "\t# return the msg or the labeled clip list\n",
    "\t# whem the validation/test data have answer\n",
    "\tif test_has_answer:\n",
    "\t\treturn classifier, t_err, v_err, t_msg, v_msg\n",
    "\t# when users do not have answer and want to get answer from the model\n",
    "\telse:\n",
    "\t\tv_msg = classifier.predict(X[t_size:])\n",
    "\t\treturn classifier, t_err, \"not valid\", t_msg, v_msg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
