{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\wizard king\n",
      "[nltk_data]     rabbit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "which method to use? (linear/RNN)linear\n",
      "enter a path to a file or a folder to add that to the training set, enter e to exitlabeled_clip_data/Teo\n",
      "enter a path to a file or a folder to add that to the training set, enter e to exitlabeled_clip_data/wardell\n",
      "enter a path to a file or a folder to add that to the training set, enter e to exite\n",
      "What proportion of the training data would be used for validation?0.2\n",
      "Do you want to use default english stopwords or stopwords given by my author? (default/author)default\n",
      "enter y to look at top 5 significant terms, enter other to quitn\n",
      "Number of training examples: [380]\n",
      "Vocabulary size: [2336]\n",
      "Training accuracy: [100.00]\n",
      "Training AUC value: [100.00]\n",
      " default accuracy: [67.37]\n",
      "\n",
      "Validation/Testing accuracy: [75.00]\n",
      "Validation/Testing AUC value: [77.78]\n",
      " default accuracy: [35.42]\n",
      "\n",
      "Do you want to save the mislabeled clips? (y/n) n\n",
      "Do you want to test this classifier on any unlabled clip data? (y/n)n\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "# this is the template learner, do not change this file but make copies and name them accordingly\n",
    "\n",
    "\n",
    "# import all you need\n",
    "import os \n",
    "from Utilities import * \n",
    "from Clip import * \n",
    "from Tokenizer_kit import * \n",
    "import pickle \n",
    "from Data_loader import *\n",
    "import log_regression\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "# ======================================= objects and methods =======================================\n",
    "\n",
    "\n",
    "def Train_and_test_learner(learner, X:list, Y, test_ratio:float) -> tuple: # train the passed learner and return filed cases \n",
    "    ''' This function should train on passed X,Y data with test_ration split \n",
    "        X should be a list of clip objects, this is for the return value\n",
    "        return is a two-tuple, first is result message with accuracies, second is list of mislabeled clips'''\n",
    "    mislabeled_clips = list() \n",
    "    train_accuracy = 0.0 \n",
    "    test_accuracy = 0.0 \n",
    "    default_accuracy = 0.0 \n",
    "    auc_score = 0.0 \n",
    "    result_msg = (f\"Total number of clips: [{len(X)}]\" + os.linesep) \n",
    "    result_msg += (f\"Train accuracy is: [{train_accuracy}]\" + os.linesep) \n",
    "    result_msg += (f\"Test accuracy is: [{test_accuracy}]\" + os.linesep) \n",
    "    result_msg += (f\"Default accuracy is: [{default_accuracy}]\" + os.linesep) \n",
    "    result_msg += (f\"AUC score is: [{auc_score}]\" + os.linesep) \n",
    "    result_msg += (f\"Number of mislabeled test clips: [{len(mislabeled_clips)}]\")\n",
    "    return (result_msg , mislabeled_clips)\n",
    "\n",
    "# please be sure that you give it a valid path when using it\n",
    "def add_filepath_to_set(the_path:str, is_file:bool, original_set):\n",
    "    if (is_file):\n",
    "        original_set.append(the_path)\n",
    "    else:\n",
    "        for entry in os.scandir(the_path):\n",
    "            if (entry.path.endswith(\".pkl\") and entry.is_file()):\n",
    "                original_set.append(entry.path)\n",
    "    return original_set\n",
    "\n",
    "# interpret a pkl file and extract its data into three lists\n",
    "def add_clipdata_to_set(clip_list, text_list, y_list, pkl_path):\n",
    "    the_file = open(pkl_path, 'rb')\n",
    "    the_pkl = pickle.load(the_file)\n",
    "    for clip in the_pkl:\n",
    "        clip_list.append(clip)\n",
    "        text_list.append(Concatenate_str_list(clip.chats))\n",
    "        if (clip.get_label_binary() == 0):\n",
    "            y_list.append(0)\n",
    "        else:\n",
    "            y_list.append(1)\n",
    "    the_file.close()\n",
    "    return clip_list, text_list, y_list\n",
    "\n",
    "\n",
    "# ====================================== end of objects and methods ====================================\n",
    "\n",
    "#def main(): \n",
    "#    while(True): # keeps prompting for and train on data \n",
    "#        print(long_line)\n",
    "#        clip_list = Prompt_for_data()\n",
    "#        if len(clip_list)==0: \n",
    "#            break  \n",
    "#        # delete this part if you want\n",
    "#        else: \n",
    "#            print(f\"Number of clips found: [{len(clip_list)}]\")  \n",
    "#\n",
    "#        # change this part \n",
    "#        msg,mislabeled = Train_and_test_learner(learner=None, X=clip_list, Y=None, test_ratio=0.2) \n",
    "#        print(f\"Printing out test result: \") \n",
    "#        print(short_line)\n",
    "#        print(msg) \n",
    "#        \n",
    "#        file_path = prompt_for_save_file(dir_path='mislabeled', f_format='.pkl') \n",
    "#        if file_path==None: \n",
    "#            continue\n",
    "#        with open(file_path, 'wb') as f: \n",
    "#            pickle.dump(mislabeled, f) \n",
    "#        print(f\"File saved as {file_path}\") \n",
    "#        continue\n",
    "#\n",
    "#    return \n",
    "\n",
    "def main(method = None): \n",
    "    # main function, a sequence of supportive methods defined above \n",
    "    # see specifications in learner_output.txt \\\n",
    "    # one good practice is to keep indent within a function no more than 3\n",
    "    # if more loop like structures are needed, another defined method is recommended\n",
    "\n",
    "    # define method\n",
    "    if method == None:\n",
    "        method = prompt_for_str(\"which method to use? (linear/RNN)\", {\"linear\", \"RNN\"})\n",
    "    #define training set\n",
    "    filepath = []\n",
    "    text = []\n",
    "    Y = []\n",
    "    all_clip = []\n",
    "    file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "    while(file_or_folder != \"e\"):\n",
    "        filepath = add_filepath_to_set(file_or_folder, _type == \"file\", filepath)\n",
    "        file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "    for filename in filepath:\n",
    "        all_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "    #define validation set\n",
    "    #filepath = []\n",
    "    #file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the validation set, enter e to exit\", {\"e\"})\n",
    "    #while(file_or_folder != \"e\"):\n",
    "    #    filepath = add_filepath_to_set(file_or_folder, _type == \"file\", filepath)\n",
    "    #    file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the validation set, enter e to exit\", {\"e\"})\n",
    "    #for filename in filepath:\n",
    "    #    all_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "    #validation_size = len(Y) - training_size\n",
    "    validation_ratio = prompt_for_float(\"What proportion of the training data would be used for validation?\", 0, 1)\n",
    "    training_size = int(len(Y) * (1 - validation_ratio))\n",
    "    validation_size = len(Y) - training_size\n",
    "    #train the model\n",
    "    if (method == \"linear\"):\n",
    "        classifier, t_err, v_err, t_msg, v_msg = log_regression.main(text, Y, training_size, validation_size)\n",
    "    if (method == \"RNN\"):\n",
    "        # call RNN method here\n",
    "        # classifier, t_err, v_err, t_msg, v_msg = ...\n",
    "        pass\n",
    "    print(t_msg)\n",
    "    print(v_msg)\n",
    "    #save the mislabeled\n",
    "    if (prompt_for_str(\"Do you want to save the mislabeled clips? (y/n) \") == \"y\"):\n",
    "        if not os.path.isdir(\"/mislabeled\"):\n",
    "            os.mkdir(\"/mislabeled\")\n",
    "        file_prefix = prompt_for_str(\"Please name the prefix of saved files: \")\n",
    "        # making mislabeled file for training errors\n",
    "        err_list = list()\n",
    "        for err_id in t_err:\n",
    "            err_list.append(all_clip[err_id])\n",
    "        new_file_path = 'mislabeled/' + file_prefix + '_mislabeled_train.pkl' \n",
    "        with open(new_file_path, 'wb') as f: \n",
    "            pickle.dump(err_list, f)\n",
    "        # making mislabeled file for validation errors\n",
    "        err_list = list()\n",
    "        for err_id in v_err:\n",
    "            err_list.append(all_clip[err_id + training_size])\n",
    "        new_file_path = 'mislabeled/' + file_prefix + '_mislabeled_validation.pkl' \n",
    "        with open(new_file_path, 'wb') as f: \n",
    "            pickle.dump(err_list, f)\n",
    "    # test the classifier\n",
    "    training_size = len(Y)\n",
    "    while (input(\"Do you want to test this classifier on any unlabled clip data? (y/n)\") == \"y\"):\n",
    "        all_clip = []\n",
    "        Y = Y[training_size:]\n",
    "        text = text[training_size:]\n",
    "        file_path = prompt_for_file(\"which file you want to do test on? \")\n",
    "        if_answer = input(\"Is this file labeled? (y/n)\") == \"y\"\n",
    "        all_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, file_path)\n",
    "        if (method == \"linear\"):\n",
    "            classifier, t_err, v_err, t_msg, v_msg = log_regression.main(text, Y, training_size, len(Y) - training_size, if_answer)\n",
    "        if (method == \"RNN\"):\n",
    "            # call RNN method here\n",
    "            # classifier, t_err, v_err, t_msg, v_msg = ...\n",
    "            pass\n",
    "        if if_answer:\n",
    "            print(v_msg)\n",
    "        else:\n",
    "            counter = 0\n",
    "            while(counter < len(all_clip)):\n",
    "                all_clip[counter].labeled = v_msg[counter]\n",
    "                counter += 1\n",
    "            file_path = prompt_for_save_file(dir_path='model_labeled_result', f_format='.pkl')\n",
    "            with open(file_path, 'wb') as f: \n",
    "                pickle.dump(all_clip, f)\n",
    "    return \n",
    "\n",
    "# ====================================== user prompts =============================================== \n",
    "\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    main() \n",
    "    exit(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
