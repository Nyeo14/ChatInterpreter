{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\wizard king\n",
      "[nltk_data]     rabbit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tokens from each review.....(can be slow for a large number of reviews)......\n",
      "show vectorizer:  CountVectorizer(min_df=0.01, ngram_range=(1, 2), stop_words='english')\n",
      "Data shape:  (651, 2116)\n",
      "['0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '1', '1', '0', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '1', '1', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '1', '0', '1', '1', '1', '1', '1', '0', '1', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '1', '1', '1', '1', '0', '1', '0', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '1', '0', '0', '1', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '0', '1', '0', '1', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '0', '1', '0', '1', '1', '0', '0', '0', '1', '1', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '1', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '0', '1', '1', '1', '0', '1', '1', '1', '0', '1', '1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '0', '1', '0', '0', '1', '1', '1', '0', '0', '0', '1', '0', '0', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '0', '1', '1', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '1', '1', '1', '1', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '0', '0', '1', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '0', '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "Number of training examples:  520\n",
      "Number of testing examples:  131\n",
      "Vocabulary size:  2116\n",
      "\n",
      "Training a model with 520 examples.....\n",
      "\n",
      "Training:\n",
      " accuracy: 99.81\n",
      "\n",
      "Testing: \n",
      " accuracy: 86.26\n",
      " AUC value: 90.22\n",
      "Positive weight rank  1 : \n",
      "---> 105 , and its weight is:  0.6292852123897608\n",
      "Positive weight rank  2 : \n",
      "---> didn , and its weight is:  0.5306346994905345\n",
      "Positive weight rank  3 : \n",
      "---> dansgame , and its weight is:  0.506319322694856\n",
      "Negative weight rank  1 : \n",
      "---> gg gg , and its weight is:  -0.5600689393000435\n",
      "Negative weight rank  2 : \n",
      "---> kekw kekw , and its weight is:  -0.5470299780276993\n",
      "Negative weight rank  3 : \n",
      "---> like , and its weight is:  -0.49508195364457175\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "import simplejson as json\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import * \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import Utilities\n",
    "import Tokenizer_kit\n",
    "import os\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# nltk.download('stopwords') is needed\n",
    "# the following functions are from HW1, with little modification.\n",
    "# \n",
    "\n",
    "def parts_of_speech(s):\n",
    "\ttokens = word_tokenize(s)\n",
    "\ttokens_and_tags = nltk.pos_tag(tokens)\n",
    "\tn = 0\n",
    "\ttemp = set()\n",
    "\tfor item in tokens_and_tags:\n",
    "\t\tif (not item[1] in temp):\n",
    "\t\t\ttemp.add(item[1])\n",
    "\t\t\tn += 1\n",
    "\tprint(\" the total number of tokens is \" + str(n))\n",
    "\ttags = [ item[1] for item in tokens_and_tags ]\n",
    "\ttag_counts = nltk.FreqDist(tags)\n",
    "\tsorted_tag_counts = tag_counts.most_common()\n",
    "\tfor item in sorted_tag_counts:\n",
    "\t\ttag_percent = 100 * item[1]/n\n",
    "\t\tp = '{0:.2f}'.format(tag_percent)\n",
    "\t\tprint('Tag:',item[0],'\\t   Percentage of tokens = ', p )\n",
    "\treturn(tokens_and_tags)\n",
    "\n",
    "def create_bow_from_reviews(clips):\n",
    "\ttext = []\n",
    "\tY = []\n",
    "\tlengths = []\n",
    "\tprint('\\nExtracting tokens from each review.....(can be slow for a large number of reviews)......')   \n",
    "\tfor clip in clips:\n",
    "\t\treview = Tokenizer_kit.Concatenate_str_list(clip.chats)\n",
    "\t\tstars = clip.get_label_binary()\n",
    "\t\tif (stars == -1):\n",
    "\t\t\tstars = 1\n",
    "\t\tif (stars == 1):\n",
    "\t\t\ttext.append(review)\n",
    "\t\t\tY.append('1')\n",
    "\t\tif (stars == 0):\n",
    "\t\t\ttext.append(review)   \n",
    "\t\t\tY.append('0')\n",
    "\tvectorizer = CountVectorizer(ngram_range = (1, 2), stop_words = 'english', min_df = 0.01)\n",
    "\tprint(\"show vectorizer: \", vectorizer)\n",
    "\tX = vectorizer.fit_transform(text)\n",
    "\tprint('Data shape: ', X.shape)\n",
    "\treturn X, Y, vectorizer\n",
    "    \n",
    "def logistic_classification(X, Y, test_fraction):\n",
    "\tX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_fraction, random_state=42)\n",
    "\tprint('Number of training examples: ', X_train.shape[0])\n",
    "\tprint('Number of testing examples: ', X_test.shape[0])   \n",
    "\tprint('Vocabulary size: ', X_train.shape[1])\n",
    "\tclassifier = linear_model.LogisticRegression(penalty = 'l2', fit_intercept = True)\n",
    "\tprint('\\nTraining a model with', X_train.shape[0], 'examples.....')\n",
    "\tclassifier.fit(X_train, Y_train)\n",
    "\ttrain_predictions = classifier.predict(X_train)\n",
    "\ttrain_accuracy = classifier.score(X_train, Y_train)\n",
    "\tprint('\\nTraining:')\n",
    "\tprint(' accuracy:',format( 100*train_accuracy , '.2f'))\n",
    "\tprint('\\nTesting: ')\n",
    "\ttest_predictions = classifier.predict(X_test)\n",
    "\ttest_accuracy = classifier.score(X_test, Y_test)\n",
    "\tprint(' accuracy:', format( 100*test_accuracy , '.2f') )\n",
    "\tclass_probabilities = classifier.predict_proba(X_test)\n",
    "\ttest_auc_score = sklearn.metrics.roc_auc_score(Y_test, class_probabilities[:,1])\n",
    "\tprint(' AUC value:', format( 100*test_auc_score , '.2f') )\n",
    "\treturn(classifier)\n",
    "\n",
    "def most_significant_terms(classifier, vectorizer, K):\n",
    "\tcount = 0\n",
    "\ttopK_pos_weights = set()\n",
    "\ttopK_pos_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmax = -1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight > 0 and weight > max and not weight in topK_pos_weights):\n",
    "\t\t\t\tmax = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not max == -1):\n",
    "\t\t\ttopK_pos_weights.add(max)\n",
    "\t\t\ttopK_pos_terms.add(temp_term)\n",
    "\t\t\tprint(\"Positive weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(max))\n",
    "\t\tcount += 1\n",
    "\tcount = 0\n",
    "\ttopK_neg_weights = set()\n",
    "\ttopK_neg_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmin = 1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight < 0 and weight < min and not weight in topK_neg_weights):\n",
    "\t\t\t\tmin = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not min == 1):\n",
    "\t\t\ttopK_neg_weights.add(min)\n",
    "\t\t\ttopK_neg_terms.add(temp_term)\n",
    "\t\t\tprint(\"Negative weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(min))\n",
    "\t\tcount += 1\n",
    "\treturn(topK_pos_weights, topK_neg_weights, topK_pos_terms, topK_neg_terms)\n",
    "\n",
    "def main():\n",
    "\tclip_list = []\n",
    "\tfor filename in os.listdir(\"labeled_clip_data\"):\n",
    "\t\tthe_file = open(\"labeled_clip_data/\" + filename, 'rb')\n",
    "\t\tthe_pkl = pickle.load(the_file)\n",
    "\t\tfor clip in the_pkl:\n",
    "\t\t\tclip_list.append(clip)\n",
    "\txx, yy, vect = create_bow_from_reviews(clip_list)\n",
    "\tprint(yy)\n",
    "\tclassifier = logistic_classification(xx, yy, 0.2)\n",
    "\tmost_significant_terms(classifier, vect, 3)\n",
    "\treturn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a path to the pickle file that storing input stringclip_data/wardell[6]_labeled.pkl\n",
      "\n",
      "Extracting tokens from each review.....(can be slow for a large number of reviews)......\n",
      "show vectorizer:  CountVectorizer(min_df=0.01, ngram_range=(1, 2), stop_words='english')\n",
      "Data shape:  (52, 6075)\n",
      "['0', '1', '0', '0', '1', '1', '1', '1', '1', '0', '1', '1', '1', '1', '1', '0', '1', '1', '0', '0', '1', '1', '1', '1', '0', '0', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '0', '1', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "Number of training examples:  41\n",
      "Number of testing examples:  11\n",
      "Vocabulary size:  6075\n",
      "\n",
      "Training a model with 41 examples.....\n",
      "\n",
      "Training:\n",
      " accuracy: 100.00\n",
      "\n",
      "Testing: \n",
      " accuracy: 100.00\n",
      " AUC value: 100.00\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
