{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\wizard king\n",
      "[nltk_data]     rabbit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a path to a file or a folder to add that to the training set, enter e to exitlabeled_clip_data/Teo\n",
      "enter a path to a file or a folder to add that to the training set, enter e to exite\n",
      "What proportion of the training data would be used for validation?0.2\\\n",
      "invalid value entered, try again\n",
      "What proportion of the training data would be used for validation?0.2\n",
      "Do you want to use default english stopwords or stopwords given by my author? (default/author)default\n",
      "Do you want to use tfidf on ohv construction? (yes/no)yes\n",
      "enter y to look at top 5 significant terms, enter other to quitn\n",
      "validation accuracy rate is -> 0.7903225806451613\n",
      "default validation accuracy rate is -> 0.7741935483870968\n",
      "Do you want to test this classifier on any unlabled clip data? (y/n)n\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# modules used\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "import simplejson as json\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import * \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from Utilities import *\n",
    "from Tokenizer_kit import *\n",
    "from Embedding import *\n",
    "from Data_loader import *\n",
    "from Data_converter import *\n",
    "from random import shuffle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# used by word purification process\n",
    "# nltk.download('stopwords') is needed\n",
    "import enchant\n",
    "global_dict = enchant.Dict(\"en_US\")\n",
    "global_slang = set({\"F\", \"???\", \"!!!\", \"!?\", \"pog\", \"nice\", \"noice\", \"haha\", \"lol\", \"lul\", \"lmao\", \"yes\", \"noo\", \"no\", \"yeah\", \"ree\", \"oof\", \"pogu\", \"xd\", \"ez\", \"money\", \"GG\", \"gg\"})\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# the following 2 functions are from HW1, with some modification.\n",
    "def logistic_classification(X, Y, classifier = None):\n",
    "\tmsg_line = \"\"\n",
    "\tif (classifier == None):\n",
    "\t\tmode = \"Training\"\n",
    "\t\tmsg_line += f\"Number of training examples: [{X.shape[0]}]\" + os.linesep\n",
    "\t\tmsg_line += f\"Vocabulary size: [{X.shape[1]}]\" + os.linesep\n",
    "\t\tclassifier = linear_model.LogisticRegression(penalty = 'l2', tol = 0.3, solver = \"sag\", max_iter = 10)\n",
    "\t\tclassifier.fit(X, Y)\n",
    "\telse:\n",
    "\t\tmode = \"Validation/Testing\"\n",
    "\taccuracy = classifier.score(X, Y)\n",
    "\tmsg_line += mode + f\" accuracy: [{format( 100*accuracy , '.2f')}]\" + os.linesep\n",
    "\ttrain_predictions = classifier.predict(X)\n",
    "\tclass_probabilities = classifier.predict_proba(X)\n",
    "\ttest_auc_score = sklearn.metrics.roc_auc_score(Y, class_probabilities[:,1])\n",
    "\tmsg_line += mode + f\" AUC value: [{format( 100*test_auc_score , '.2f')}]\" + os.linesep\n",
    "\tdefault_counter = 0\n",
    "\tfor i in Y:\n",
    "\t\tif (i == 0):\n",
    "\t\t\tdefault_counter += 1\n",
    "\tdefault_accuracy = default_counter / len(Y)\n",
    "\tmsg_line += f\" default accuracy: [{format( 100*default_accuracy , '.2f')}]\" + os.linesep\n",
    "\tcounter = 0\n",
    "\tmy_error = []\n",
    "\twhile (counter < X.shape[0]):\n",
    "\t\tif (train_predictions[counter] != Y[counter]):\n",
    "\t\t\tmy_error.append(counter)\n",
    "\t\tcounter += 1\n",
    "\treturn classifier, my_error, msg_line\n",
    "\n",
    "def most_significant_terms(classifier, vectorizer, K):\n",
    "\tcount = 0\n",
    "\ttopK_pos_weights = set()\n",
    "\ttopK_pos_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmax = -1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight > 0 and weight > max and not weight in topK_pos_weights):\n",
    "\t\t\t\tmax = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not max == -1):\n",
    "\t\t\ttopK_pos_weights.add(max)\n",
    "\t\t\ttopK_pos_terms.add(temp_term)\n",
    "\t\t\tprint(\"Positive weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(max))\n",
    "\t\tcount += 1\n",
    "\tcount = 0\n",
    "\ttopK_neg_weights = set()\n",
    "\ttopK_neg_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmin = 1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight < 0 and weight < min and not weight in topK_neg_weights):\n",
    "\t\t\t\tmin = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not min == 1):\n",
    "\t\t\ttopK_neg_weights.add(min)\n",
    "\t\t\ttopK_neg_terms.add(temp_term)\n",
    "\t\t\tprint(\"Negative weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(min))\n",
    "\t\tcount += 1\n",
    "\treturn(topK_pos_weights, topK_neg_weights, topK_pos_terms, topK_neg_terms)\n",
    "\n",
    "# train log regression model with the data provided\n",
    "def train_log_regression(X, Y):\n",
    "\tclassifier = linear_model.LogisticRegression(penalty = 'l2', tol = 0.3, solver = \"sag\", max_iter = 10)\n",
    "\tclassifier.fit(X, Y)\n",
    "\treturn classifier\n",
    "\n",
    "# get validation error rate and default error rate of a model with the data provided\n",
    "def validate(classifier, X, Y):\n",
    "\taccuracy = classifier.score(X, Y)\n",
    "\tdefault_counter = 0\n",
    "\tfor i in Y:\n",
    "\t\tif (i == 0):\n",
    "\t\t\tdefault_counter += 1\n",
    "\tdefault_accuracy = default_counter / len(Y)\n",
    "\treturn accuracy, default_accuracy\n",
    "\n",
    "# directly convert a list of long strings into a one-hot vector\n",
    "# it does both tokenization and vectorization\n",
    "# it should returns an 2-D array\n",
    "# X index the clip, Y index the token\n",
    "# However, since countVectorizer gives a better result (it does a better job tokenizing stuff)\n",
    "# this function is not used\n",
    "def to_ohv(text_list, stop_words = [], min_len = 2):\n",
    "\ttoken_set = set()\n",
    "\tfor text in text_list:\n",
    "\t\tfor word in text.split():\n",
    "\t\t\tif (len(word) > min_len) and (not word in stop_words) and (not word in set(stopwords.words('english'))) and (not word in token_set):\n",
    "\t\t\t\ttoken_set.add(word)\n",
    "\tresult = np.zeros((len(text_list), len(token_set)))\n",
    "\tfor i, token in enumerate(token_set):\n",
    "\t\tfor ii, sentence in enumerate(text_list):\n",
    "\t\t\tif (token in sentence):\n",
    "\t\t\t\tresult[ii][i] = 1\n",
    "\treturn result\n",
    "\n",
    "# translate a word into something standard\n",
    "def my_translator(target_word, stop_words = nltk_stop_words):\n",
    "\tresult = target_word\n",
    "\tif global_dict.check(target_word):\n",
    "\t\t# this word is a standard word, return it\n",
    "\t\tresult =  target_word\n",
    "\telif (target_word in global_slang):\n",
    "\t\t# this word is not a standard word, is it an internet slang?\n",
    "\t\tresult =  target_word\n",
    "\telif (Embedding_word_modifier(target_word) in global_slang):\n",
    "\t\t# Or it could be some special form of an iternet slang\n",
    "\t\tresult =  Embedding_word_modifier(target_word)\n",
    "\telif len(global_dict.suggest(target_word)) > 0:\n",
    "\t\t# it is nothing but there are similar words\n",
    "\t\tresult =  global_dict.suggest(target_word)[0]\n",
    "\telse:\n",
    "\t\t# it is nothing, probably an emote\n",
    "\t\t# but we do not have a similar word to it, so return itself\n",
    "\t\tresult = target_word\n",
    "\ttry:\n",
    "\t\t# is it a number? Maybe we should purify numbers\n",
    "\t\t_test = float(target_word)\n",
    "\t\tresult = \"NUMBER_WORD\"\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\tif result == None:\n",
    "\t\tresult =  target_word\n",
    "\telse:\n",
    "\t\tresult =  result\n",
    "\t# last step : remove redundant consequtive words\n",
    "\treal_result = []\n",
    "\tlast_letter = None\n",
    "\tfor letter in result:\n",
    "\t\tif (not last_letter == None) and (letter == last_letter):\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\treal_result.append(letter)\n",
    "\t\tlast_letter = letter\n",
    "\treturn Concatenate_str_list(real_result, splitter = '')\n",
    "\n",
    "# simple version of my translator. there are fewer steps.\n",
    "def my_translator_simple(target_word, stop_words = nltk_stop_words):\n",
    "\tresult = target_word\n",
    "\tif global_dict.check(target_word):\n",
    "\t\t# this word is a standard word, return it\n",
    "\t\tresult =  target_word\n",
    "\telif (target_word in global_slang):\n",
    "\t\t# this word is not a standard word, is it an internet slang?\n",
    "\t\tresult =  target_word\n",
    "\telif (Embedding_word_modifier(target_word) in global_slang):\n",
    "\t\t# Or it could be some special form of an iternet slang\n",
    "\t\tresult =  Embedding_word_modifier(target_word)\n",
    "\telif len(global_dict.suggest(target_word)) > 0:\n",
    "\t\t# it is nothing but there are similar words\n",
    "\t\tresult =  global_dict.suggest(target_word)[0]\n",
    "\telse:\n",
    "\t\t# it is nothing, probably an emote\n",
    "\t\t# but we do not have a similar word to it, so return itself\n",
    "\t\tresult = target_word\n",
    "\treturn result\n",
    "        \n",
    "# utility function, you give a path to it and it add all .pkl file under that path to original set\n",
    "# please be sure that you give it a valid path when using it\n",
    "def add_filepath_to_set(the_path:str, is_file:bool, original_set):\n",
    "    if (is_file):\n",
    "        original_set.append(the_path)\n",
    "    else:\n",
    "        for entry in os.scandir(the_path):\n",
    "            if (entry.path.endswith(\".pkl\") and entry.is_file()):\n",
    "                original_set.append(entry.path)\n",
    "    return original_set\n",
    "\n",
    "# interpret a pkl file and extract its data into three lists\n",
    "def add_clipdata_to_set(clip_list, text_list, y_list, pkl_path, do_convert = 0, filter_stopword = True, show_debug = False):\n",
    "\tthe_file = open(pkl_path, 'rb')\n",
    "\tthe_pkl = pickle.load(the_file)\n",
    "\tfor clip in the_pkl:\n",
    "\t\tclip_list.append(clip)\n",
    "\t\tif do_convert == 0:\n",
    "\t\t\t# no conversion, totally original text\n",
    "\t\t\ttext_list.append(Concatenate_str_list(clip.chats))\n",
    "\t\telif do_convert == 1:\n",
    "\t\t\t# massive translation\n",
    "\t\t\ttemp_text = []\n",
    "\t\t\tfor chat in clip.chats:\n",
    "\t\t\t\tfor word in chat.split():\n",
    "\t\t\t\t\ttemp_word = my_translator(word)\n",
    "\t\t\t\t\ttemp_text.append(temp_word)\n",
    "\t\t\tif (show_debug):\n",
    "\t\t\t\tprint(temp_text)\n",
    "\t\t\ttext_list.append(Concatenate_str_list(temp_text))\n",
    "\t\telif do_convert == 2:\n",
    "\t\t\t# minimal translation, faster\n",
    "\t\t\ttemp_text = []\n",
    "\t\t\tfor chat in clip.chats:\n",
    "\t\t\t\tfor word in chat.split():\n",
    "\t\t\t\t\ttemp_word = my_translator_simple(word)\n",
    "\t\t\t\t\ttemp_text.append(temp_word)\n",
    "\t\t\tif (show_debug):\n",
    "\t\t\t\tprint(temp_text)\n",
    "\t\t\ttext_list.append(Concatenate_str_list(temp_text))\n",
    "\t\tif (clip.get_label_binary() == 0):\n",
    "\t\t\ty_list.append(0)\n",
    "\t\telse:\n",
    "\t\t\ty_list.append(1)\n",
    "\tthe_file.close()\n",
    "\treturn clip_list, text_list, y_list\n",
    "\n",
    "# randomize data\n",
    "def randomize_data(clip_list, text_list, y_list):\n",
    "\torder_list = list(range(len(clip_list)))\n",
    "\trandom.shuffle(order_list)\n",
    "\tnew_clip = list(clip_list)\n",
    "\tnew_text = list(text_list)\n",
    "\tnew_y = list(y_list)\n",
    "\tfor i, ii in enumerate(order_list):\n",
    "\t\tnew_clip[i] = clip_list[ii]\n",
    "\t\tnew_text[i] = text_list[ii]\n",
    "\t\tnew_y[i] = y_list[ii]\n",
    "\treturn new_clip, new_text, new_y\n",
    "\n",
    "# this function iteratively run the main to find the best param\n",
    "def best_param(ngram, panelty, dual, tol, C, fit_intercept, solver, max_iter, num_iter = 10, test_ratio = 0.2, test_on = [\"labeled_clip_data/Teo\"]):\n",
    "\tva_err_list = []\n",
    "\t#define training set\n",
    "\tfilepath = []\n",
    "\ttext = []\n",
    "\tY = []\n",
    "\tall_clip = []\n",
    "\tfor path in test_on:\n",
    "\t\tfilepath = add_filepath_to_set(path, False, filepath)\n",
    "\tfor filename in filepath:\n",
    "\t\tall_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "\t#define validation set\n",
    "\ttraining_size = int(len(Y) * (1 - test_ratio))\n",
    "\tvalidation_size = len(Y) - training_size\n",
    "\t# iteratively test the model\n",
    "\twhile (num_iter > 0):\n",
    "\t\t# randomize the data\n",
    "\t\tall_clip, text, Y = randomize_data(all_clip, text, Y)\n",
    "\t\t# construct the vectorizer\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, ngram), stop_words = 'english', min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\t\tX = vect.fit_transform(text)\n",
    "\t\t# make classifier\n",
    "\t\t# the following line is responsible for taking different parameters\n",
    "\t\tclassifier = linear_model.LogisticRegression(C = C, dual = dual, penalty = panelty, fit_intercept = fit_intercept, tol = tol, solver = solver, max_iter = max_iter)\n",
    "\t\tclassifier.fit(X[:training_size], Y[:training_size])\n",
    "\t\tva_err_list.append(classifier.score(X[training_size:], Y[training_size:]))\n",
    "\t\tnum_iter -= 1\n",
    "\treturn np.average(va_err_list)\n",
    "\n",
    "# main\n",
    "# you do not need to run sudo main if you already have proper input data\n",
    "def main():\n",
    "\ttext = []\n",
    "\tY = []\n",
    "\tall_clip = []\n",
    "\t# define data set\n",
    "\tfilepath = []\n",
    "\tfile_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "\twhile(file_or_folder != \"e\"):\n",
    "\t\tfilepath = add_filepath_to_set(file_or_folder, _type == \"file\", filepath)\n",
    "\t\tfile_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "\tfor filename in filepath:\n",
    "\t\tall_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "\t#define validation set\n",
    "\tvalidation_ratio = prompt_for_float(\"What proportion of the training data would be used for validation?\", 0, 1)\n",
    "\ttraining_size = int(len(Y) * (1 - validation_ratio))\n",
    "\tvalidation_size = len(Y) - training_size\n",
    "\t# randomize the data\n",
    "\tall_clip, text, Y = randomize_data(all_clip, text, Y)\n",
    "\t# define stop word\n",
    "\tif_stop = prompt_for_str(\"Do you want to use default english stopwords or stopwords given by my author? (default/author)\", {\"default\",\"author\"})\n",
    "\tif (if_stop == \"default\"):\n",
    "\t\tspecial_stop_word = set(stopwords.words('english'))\n",
    "\tif (if_stop == \"author\"):\n",
    "\t\tspecial_stop_word = global_slang\n",
    "\t# construct the vectorizer\n",
    "\tif_tfidf = prompt_for_str(\"Do you want to use tfidf on ohv construction? (yes/no)\", {\"yes\",\"no\"})\n",
    "\tif (if_tfidf == \"yes\"):\n",
    "\t\tvect = TfidfVectorizer(ngram_range = (1, 2), stop_words = special_stop_word, min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\tif (if_tfidf == \"no\"):\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, 2), stop_words = special_stop_word, min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\tX = vect.fit_transform(text)\n",
    "\t# make classifier\n",
    "\tclassifier = train_log_regression(X[:training_size], Y[:training_size])\n",
    "\taccu, default_accu = validate(classifier, X[training_size:], Y[training_size:])\n",
    "\t# look at result\n",
    "\tif (input(\"enter y to look at top 5 significant terms, enter other to quit\") == \"y\"):\n",
    "\t\tmost_significant_terms(classifier, vect, 5)\n",
    "\tprint (\"validation accuracy rate is -> \" + str(accu))\n",
    "\tprint (\"default validation accuracy rate is -> \" + str(default_accu))\n",
    "\t# test the classifier\n",
    "\ttraining_size = len(Y)\n",
    "\twhile (input(\"Do you want to test this classifier on any unlabled clip data? (y/n)\") == \"y\"):\n",
    "\t\ttraining_size = len(Y)\n",
    "\t\tfile_path = prompt_for_file(\"which file you want to do test on? \")\n",
    "\t\tall_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, file_path)\n",
    "\t\tif (if_tfidf == \"yes\"):\n",
    "\t\t\tvect = TfidfVectorizer(ngram_range = (1, 2), stop_words = special_stop_word, min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\t\tif (if_tfidf == \"no\"):\n",
    "\t\t\tvect = CountVectorizer(ngram_range = (1, 2), stop_words = special_stop_word, min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\t\tX = vect.fit_transform(text)\n",
    "\t\tclassifier = train_log_regression(X[:training_size], Y[:training_size])\n",
    "\t\tpredictions = classifier.predict(X[training_size:])\n",
    "\t\tcounter = 0\n",
    "\t\twhile(counter < len(all_clip)):\n",
    "\t\t\tall_clip[counter].labeled = predictions[counter]\n",
    "\t\t\tcounter += 1\n",
    "\t\tfile_path = prompt_for_save_file(dir_path='model_labeled_result', f_format='.pkl')\n",
    "\t\twith open(file_path, 'wb') as f: \n",
    "\t\t\tpickle.dump(all_clip, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
