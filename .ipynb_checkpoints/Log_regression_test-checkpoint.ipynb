{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\wizard king\n",
      "[nltk_data]     rabbit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a path to a file or a folder to add that to the training set, enter e to exitlabeled_clip_data/Teo\n",
      "enter a path to a file or a folder to add that to the training set, enter e to exite\n",
      "What proportion of the training data would be used for validation?0.2\n",
      "Do you want to use default english stopwords or stopwords given by my author? (default/author)default\n",
      "enter y to look at top 5 significant terms, enter other to quitn\n",
      "Number of training examples: [245]\n",
      "Vocabulary size: [2194]\n",
      "Training accuracy: [100.00]\n",
      "Training AUC value: [100.00]\n",
      " default accuracy: [75.51]\n",
      "\n",
      "Validation/Testing accuracy: [88.71]\n",
      "Validation/Testing AUC value: [95.32]\n",
      " default accuracy: [74.19]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "import simplejson as json\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import * \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from Utilities import *\n",
    "from Tokenizer_kit import *\n",
    "from Embedding import *\n",
    "from Data_loader import *\n",
    "from Data_converter import *\n",
    "from random import shuffle\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# nltk.download('stopwords') is needed\n",
    "# the following 2 functions are from HW1, with some modification.\n",
    "def logistic_classification(X, Y, classifier = None):\n",
    "\tmsg_line = \"\"\n",
    "\tif (classifier == None):\n",
    "\t\tmode = \"Training\"\n",
    "\t\tmsg_line += f\"Number of training examples: [{X.shape[0]}]\" + os.linesep\n",
    "\t\tmsg_line += f\"Vocabulary size: [{X.shape[1]}]\" + os.linesep\n",
    "\t\tclassifier = linear_model.LogisticRegression(penalty = 'l2', tol = 0.05, solver = \"liblinear\", max_iter = 150)\n",
    "\t\tclassifier.fit(X, Y)\n",
    "\telse:\n",
    "\t\tmode = \"Validation/Testing\"\n",
    "\taccuracy = classifier.score(X, Y)\n",
    "\tmsg_line += mode + f\" accuracy: [{format( 100*accuracy , '.2f')}]\" + os.linesep\n",
    "\ttrain_predictions = classifier.predict(X)\n",
    "\tclass_probabilities = classifier.predict_proba(X)\n",
    "\ttest_auc_score = sklearn.metrics.roc_auc_score(Y, class_probabilities[:,1])\n",
    "\tmsg_line += mode + f\" AUC value: [{format( 100*test_auc_score , '.2f')}]\" + os.linesep\n",
    "\tdefault_accuracy = classifier.score(X, np.zeros(len(Y)))\n",
    "\tmsg_line += f\" default accuracy: [{format( 100*default_accuracy , '.2f')}]\" + os.linesep\n",
    "\tcounter = 0\n",
    "\tmy_error = []\n",
    "\twhile (counter < X.shape[0]):\n",
    "\t\tif (train_predictions[counter] != Y[counter]):\n",
    "\t\t\tmy_error.append(counter)\n",
    "\t\tcounter += 1\n",
    "\treturn classifier, my_error, msg_line\n",
    "\n",
    "def most_significant_terms(classifier, vectorizer, K):\n",
    "\tcount = 0\n",
    "\ttopK_pos_weights = set()\n",
    "\ttopK_pos_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmax = -1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight > 0 and weight > max and not weight in topK_pos_weights):\n",
    "\t\t\t\tmax = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not max == -1):\n",
    "\t\t\ttopK_pos_weights.add(max)\n",
    "\t\t\ttopK_pos_terms.add(temp_term)\n",
    "\t\t\tprint(\"Positive weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(max))\n",
    "\t\tcount += 1\n",
    "\tcount = 0\n",
    "\ttopK_neg_weights = set()\n",
    "\ttopK_neg_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmin = 1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight < 0 and weight < min and not weight in topK_neg_weights):\n",
    "\t\t\t\tmin = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not min == 1):\n",
    "\t\t\ttopK_neg_weights.add(min)\n",
    "\t\t\ttopK_neg_terms.add(temp_term)\n",
    "\t\t\tprint(\"Negative weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(min))\n",
    "\t\tcount += 1\n",
    "\treturn(topK_pos_weights, topK_neg_weights, topK_pos_terms, topK_neg_terms)\n",
    "\n",
    "# directly convert a list of long strings into a one-hot vector\n",
    "# it does both tokenization and vectorization\n",
    "# it should returns an 2-D array\n",
    "# X index the clip, Y index the token\n",
    "def to_ohv(text_list, stop_words = [], min_len = 2):\n",
    "\ttoken_set = set()\n",
    "\tfor text in text_list:\n",
    "\t\tfor word in text.split():\n",
    "\t\t\tif (len(word) > min_len) and (not word in stop_words) and (not word in set(stopwords.words('english'))) and (not word in token_set):\n",
    "\t\t\t\ttoken_set.add(word)\n",
    "\tresult = np.zeros((len(text_list), len(token_set)))\n",
    "\tfor i, token in enumerate(token_set):\n",
    "\t\tfor ii, sentence in enumerate(text_list):\n",
    "\t\t\tif (token in sentence):\n",
    "\t\t\t\tresult[ii][i] = 1\n",
    "\treturn result\n",
    "\n",
    "# translate a word into something standard\n",
    "def my_translator(target_word):\n",
    "\tpass\n",
    "\n",
    "# please be sure that you give it a valid path when using it\n",
    "def add_filepath_to_set(the_path:str, is_file:bool, original_set):\n",
    "    if (is_file):\n",
    "        original_set.append(the_path)\n",
    "    else:\n",
    "        for entry in os.scandir(the_path):\n",
    "            if (entry.path.endswith(\".pkl\") and entry.is_file()):\n",
    "                original_set.append(entry.path)\n",
    "    return original_set\n",
    "\n",
    "# interpret a pkl file and extract its data into three lists\n",
    "def add_clipdata_to_set(clip_list, text_list, y_list, pkl_path):\n",
    "    the_file = open(pkl_path, 'rb')\n",
    "    the_pkl = pickle.load(the_file)\n",
    "    for clip in the_pkl:\n",
    "        clip_list.append(clip)\n",
    "        text_list.append(Concatenate_str_list(clip.chats))\n",
    "        if (clip.get_label_binary() == 0):\n",
    "            y_list.append(0)\n",
    "        else:\n",
    "            y_list.append(1)\n",
    "    the_file.close()\n",
    "    return clip_list, text_list, y_list\n",
    "\n",
    "# randomize data\n",
    "def randomize_data(clip_list, text_list, y_list):\n",
    "\torder_list = list(range(len(clip_list)))\n",
    "\trandom.shuffle(order_list)\n",
    "\tnew_clip = list(clip_list)\n",
    "\tnew_text = list(text_list)\n",
    "\tnew_y = list(y_list)\n",
    "\tfor i, ii in enumerate(order_list):\n",
    "\t\tnew_clip[i] = clip_list[ii]\n",
    "\t\tnew_text[i] = text_list[ii]\n",
    "\t\tnew_y[i] = y_list[ii]\n",
    "\treturn new_clip, new_text, new_y\n",
    "\n",
    "# this function iteratively run the main to find the best param\n",
    "def best_param(panelty, dual, tol, C, fit_intercept, solver, max_iter, num_iter = 10, test_ratio = 0.2, test_on = [\"labeled_clip_data/Teo\", \"labeled_clip_data/wardell\", \"labeled_clip_data/T90\"]):\n",
    "\tva_err_list = []\n",
    "\t#define training set\n",
    "\tfilepath = []\n",
    "\ttext = []\n",
    "\tY = []\n",
    "\tall_clip = []\n",
    "\tfor path in test_on:\n",
    "\t\tfilepath = add_filepath_to_set(path, False, filepath)\n",
    "\tfor filename in filepath:\n",
    "\t\tall_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "\t#define validation set\n",
    "\ttraining_size = int(len(Y) * (1 - test_ratio))\n",
    "\tvalidation_size = len(Y) - training_size\n",
    "\t# iteratively test the model\n",
    "\twhile (num_iter > 0):\n",
    "\t\t# randomize the data\n",
    "\t\tall_clip, text, Y = randomize_data(all_clip, text, Y)\n",
    "\t\t# construct the vectorizer\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, 2), stop_words = 'english', min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\t\tX = vect.fit_transform(text)\n",
    "\t\t# make classifier\n",
    "\t\t# the following line is responsible for taking different parameters\n",
    "\t\tclassifier = linear_model.LogisticRegression(C = C, dual = dual, penalty = panelty, fit_intercept = fit_intercept, tol = tol, solver = solver, max_iter = max_iter)\n",
    "\t\tclassifier.fit(X[:training_size], Y[:training_size])\n",
    "\t\tva_err_list.append(classifier.score(X[training_size:], Y[training_size:]))\n",
    "\t\tnum_iter -= 1\n",
    "\treturn np.average(va_err_list)\n",
    "\n",
    "def sudo_main(ask_save = True, ask_test = True, if_debug = True): \n",
    "    # main function, a sequence of supportive methods defined above \n",
    "    # see specifications in learner_output.txt \\\n",
    "    # one good practice is to keep indent within a function no more than 3\n",
    "    # if more loop like structures are needed, another defined method is recommended\n",
    "\n",
    "    #define training set\n",
    "    text = []\n",
    "    Y = []\n",
    "    all_clip = []\n",
    "    filepath = []\n",
    "    file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "    while(file_or_folder != \"e\"):\n",
    "        filepath = add_filepath_to_set(file_or_folder, _type == \"file\", filepath)\n",
    "        file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "    for filename in filepath:\n",
    "        all_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "    #define validation set\n",
    "    validation_ratio = prompt_for_float(\"What proportion of the training data would be used for validation?\", 0, 1)\n",
    "    training_size = int(len(Y) * (1 - validation_ratio))\n",
    "    validation_size = len(Y) - training_size\n",
    "    # randomize the data\n",
    "    all_clip, text, Y = randomize_data(all_clip, text, Y)\n",
    "    # train the model\n",
    "    classifier, t_err, v_err, t_msg, v_msg = main(text, Y, training_size, validation_size)\n",
    "    if if_debug:\n",
    "        print(t_msg)\n",
    "        print(v_msg)\n",
    "    #save the mislabeled\n",
    "    if (ask_save and prompt_for_str(\"Do you want to save the mislabeled clips? (y/n) \") == \"y\"):\n",
    "        if not os.path.isdir(\"/mislabeled\"):\n",
    "            os.mkdir(\"/mislabeled\")\n",
    "        file_prefix = prompt_for_str(\"Please name the prefix of saved files: \")\n",
    "        # making mislabeled file for training errors\n",
    "        err_list = list()\n",
    "        for err_id in t_err:\n",
    "            err_list.append(all_clip[err_id])\n",
    "        new_file_path = 'mislabeled/' + file_prefix + '_mislabeled_train.pkl' \n",
    "        with open(new_file_path, 'wb') as f: \n",
    "            pickle.dump(err_list, f)\n",
    "        # making mislabeled file for validation errors\n",
    "        err_list = list()\n",
    "        for err_id in v_err:\n",
    "            err_list.append(all_clip[err_id + training_size])\n",
    "        new_file_path = 'mislabeled/' + file_prefix + '_mislabeled_validation.pkl' \n",
    "        with open(new_file_path, 'wb') as f: \n",
    "            pickle.dump(err_list, f)\n",
    "    # test the classifier\n",
    "    training_size = len(Y)\n",
    "    while (ask_test and input(\"Do you want to test this classifier on any unlabled clip data? (y/n)\") == \"y\"):\n",
    "        all_clip = []\n",
    "        Y = Y[training_size:]\n",
    "        text = text[training_size:]\n",
    "        file_path = prompt_for_file(\"which file you want to do test on? \")\n",
    "        if_answer = input(\"Is this file labeled? (y/n)\") == \"y\"\n",
    "        all_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, file_path)\n",
    "        classifier, t_err, v_err, t_msg, v_msg = main(text, Y, training_size, len(Y) - training_size, if_answer)\n",
    "        if if_answer:\n",
    "            print(v_msg)\n",
    "        else:\n",
    "            counter = 0\n",
    "            while(counter < len(all_clip)):\n",
    "                all_clip[counter].labeled = v_msg[counter]\n",
    "                counter += 1\n",
    "            file_path = prompt_for_save_file(dir_path='model_labeled_result', f_format='.pkl')\n",
    "            with open(file_path, 'wb') as f: \n",
    "                pickle.dump(all_clip, f)\n",
    "    return classifier.score(text[:training_size], Y[:training_size]), classifier.score(text[training_size:], Y[training_size:])\n",
    "\n",
    "# main\n",
    "def main(the_text = None, the_y = None, t_size = None, v_size = None, test_has_answer = True, always_default = False):\n",
    "\tif (the_text == None):\n",
    "\t\tsudo_main()\n",
    "\t\treturn\n",
    "\t# define stop word\n",
    "\tif (not always_default):\n",
    "\t\tif_stop = prompt_for_str(\"Do you want to use default english stopwords or stopwords given by my author? (default/author)\", {\"default\",\"author\"})\n",
    "\t\tspecial_stop_word = None\n",
    "\t\tif (if_stop == \"default\"):\n",
    "\t\t\tpass\n",
    "\t\tif (if_stop == \"author\"):\n",
    "\t\t\tspecial_stop_word = {\"1\", \"2\", \"11\", \"111111\", \"gg\", \"gg gg\", \"LUL\", \"LOL\"}\n",
    "\t# construct the vectorizer\n",
    "\tif (special_stop_word == None):\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, 2), stop_words = 'english', min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\telse:\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, 2), stop_words = special_stop_word, min_df = 0.01,  tokenizer = Embedding_tokenize)\n",
    "\tX = vect.fit_transform(the_text)\n",
    "\t#X = to_ohv(the_text)\n",
    "\t# make classifier\n",
    "\tclassifier, t_err, t_msg = logistic_classification(X[:t_size], the_y[:t_size])\n",
    "\tif test_has_answer:\n",
    "\t\t_c, v_err, v_msg = logistic_classification(X[t_size:], the_y[t_size:], classifier)\n",
    "\t# look at result\n",
    "\tif ((not always_default) and input(\"enter y to look at top 5 significant terms, enter other to quit\") == \"y\"):\n",
    "\t\tmost_significant_terms(classifier, vect, 5)\n",
    "\t# return the msg or the labeled clip list\n",
    "\t# whem the validation/test data have answer\n",
    "\tif test_has_answer:\n",
    "\t\treturn classifier, t_err, v_err, t_msg, v_msg\n",
    "\t# when users do not have answer and want to get answer from the model\n",
    "\telse:\n",
    "\t\tv_msg = classifier.predict(X[t_size:])\n",
    "\t\treturn classifier, t_err, \"not valid\", t_msg, v_msg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9879098360655737\n",
      "0.9889344262295081\n",
      "0.993032786885246\n",
      "0.9911885245901638\n",
      "0.9901639344262294\n",
      "0.9899590163934426\n",
      "0.9911885245901638\n",
      "0.9889344262295081\n",
      "0.9948770491803278\n",
      "0.9907786885245902\n"
     ]
    }
   ],
   "source": [
    "print(best_param('l2', True, 0.01, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.02, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.08, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.1, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.15, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.2, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.3, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.4, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.5, 1, True, 'liblinear', 50, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9848360655737706\n",
      "0.9895491803278688\n",
      "0.9887295081967213\n",
      "0.9942622950819672\n",
      "0.9922131147540985\n",
      "0.9924180327868852\n",
      "0.9924180327868852\n",
      "0.9936475409836065\n",
      "0.9905737704918032\n",
      "0.9907786885245903\n"
     ]
    }
   ],
   "source": [
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 10, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 20, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 30, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 40, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 80, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 100, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 150, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 200, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 250, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9901639344262294\n",
      "0.9907786885245902\n",
      "0.9928278688524589\n",
      "0.9903688524590164\n",
      "0.9938524590163935\n",
      "0.9928278688524591\n",
      "0.9940573770491803\n",
      "0.9907786885245902\n",
      "0.9891393442622951\n",
      "0.9897540983606558\n"
     ]
    }
   ],
   "source": [
    "print(best_param('l2', True, 0.05, 0.1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 0.2, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 0.3, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 0.4, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 0.5, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 0.6, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 0.7, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 0.8, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 0.9, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.990983606557377\n",
      "0.9401639344262296\n",
      "0.9155737704918032\n",
      "0.9899590163934426\n",
      "0.9903688524590164\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d3a867e5350c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'l1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'liblinear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'l1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'saga'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-c9373a3d0b5c>\u001b[0m in \u001b[0;36mbest_param\u001b[1;34m(panelty, dual, tol, C, fit_intercept, solver, max_iter, num_iter, test_ratio, test_on)\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[1;31m# the following line is responsible for taking different parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                 \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpanelty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_intercept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_intercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m                 \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtraining_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtraining_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m                 \u001b[0mva_err_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0mnum_iter\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1354\u001b[0m                               \u001b[1;34m\" 'solver' is set to 'liblinear'. Got 'n_jobs'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m                               \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n\u001b[1;32m-> 1356\u001b[1;33m             self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[0;32m   1357\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_scaling\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    963\u001b[0m                                          dtype=np.float64)\n\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m     \u001b[0msolver_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0;32m    967\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_get_liblinear_solver_type\u001b[1;34m(multi_class, penalty, loss, dual)\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0msolver_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m     raise ValueError('Unsupported set of arguments: %s, '\n\u001b[0m\u001b[0;32m    822\u001b[0m                      \u001b[1;34m'Parameters: penalty=%r, loss=%r, dual=%r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m                      % (error_string, penalty, loss, dual))\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True"
     ]
    }
   ],
   "source": [
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', False, 0.05, 1, True, 'sag', 50, 20, 0.4))\n",
    "print(best_param('l2', False, 0.05, 1, True, 'saga', 50, 20, 0.4))\n",
    "print(best_param('l2', False, 0.05, 1, True, 'lbfgs', 50, 20, 0.4))\n",
    "print(best_param('l2', False, 0.05, 1, True, 'newton-cg', 50, 20, 0.4))\n",
    "print(best_param('l1', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l1', False, 0.05, 1, True, 'saga', 50, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9920081967213115\n",
      "0.989549180327869\n",
      "0.9868852459016393\n",
      "0.9903688524590164\n"
     ]
    }
   ],
   "source": [
    "print(best_param('l2', True, 0.05, 1, False, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', False, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', False, 0.05, 1, False, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9914634146341463\n",
      "0.9878048780487806\n",
      "0.990650406504065\n",
      "0.994308943089431\n",
      "0.9922764227642276\n",
      "0.993089430894309\n",
      "0.9934959349593496\n",
      "0.9914634146341463\n",
      "0.990650406504065\n",
      "0.9926829268292682\n"
     ]
    }
   ],
   "source": [
    "print(best_param('l2', True, 0.01, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))\n",
    "print(best_param('l2', True, 0.02, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))\n",
    "print(best_param('l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))\n",
    "print(best_param('l2', True, 0.08, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))\n",
    "print(best_param('l2', True, 0.1, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))\n",
    "print(best_param('l2', True, 0.15, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))\n",
    "print(best_param('l2', True, 0.2, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))\n",
    "print(best_param('l2', True, 0.3, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))\n",
    "print(best_param('l2', True, 0.4, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))\n",
    "print(best_param('l2', True, 0.5, 1, True, 'liblinear', 50, 20, 0.4, test_on = [\"labeled_clip_data/Teo\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a path to a file or a folder to add that to the training set, enter e to exitlabeled_clip_data/Teo\n",
      "enter a path to a file or a folder to add that to the training set, enter e to exite\n",
      "What proportion of the training data would be used for validation?0.2\n",
      "Do you want to use default english stopwords or stopwords given by my author? (default/author)default\n",
      "enter y to look at top 5 significant terms, enter other to quity\n",
      "Positive weight rank  1 : \n",
      "---> pog , and its weight is:  0.46457452798146126\n",
      "Positive weight rank  2 : \n",
      "---> pogu , and its weight is:  0.3499507494111514\n",
      "Positive weight rank  3 : \n",
      "---> loud , and its weight is:  0.31106870330992475\n",
      "Positive weight rank  4 : \n",
      "---> god , and its weight is:  0.2607254018804821\n",
      "Positive weight rank  5 : \n",
      "---> mirror , and its weight is:  0.2454285111797535\n",
      "Negative weight rank  1 : \n",
      "---> teosgame , and its weight is:  -0.32127776062612545\n",
      "Negative weight rank  2 : \n",
      "---> corsair , and its weight is:  -0.29470265519179856\n",
      "Negative weight rank  3 : \n",
      "---> yeah , and its weight is:  -0.27001465801873464\n",
      "Negative weight rank  4 : \n",
      "---> like , and its weight is:  -0.26866327615885344\n",
      "Negative weight rank  5 : \n",
      "---> lol , and its weight is:  -0.2335960016772691\n",
      "Number of training examples: [245]\n",
      "Vocabulary size: [18781]\n",
      "Training accuracy: [100.00]\n",
      "Training AUC value: [100.00]\n",
      " default accuracy: [76.33]\n",
      "\n",
      "Validation/Testing accuracy: [95.16]\n",
      "Validation/Testing AUC value: [97.25]\n",
      " default accuracy: [77.42]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-e2e2557c97d5>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(the_text, the_y, t_size, v_size, test_has_answer, always_default)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthe_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_has_answer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malways_default\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mthe_text\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m                 \u001b[0msudo_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;31m# define stop word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-e2e2557c97d5>\u001b[0m in \u001b[0;36msudo_main\u001b[1;34m(ask_save, ask_test, if_debug)\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;31m#save the mislabeled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mask_save\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mprompt_for_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Do you want to save the mislabeled clips? (y/n) \"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/mislabeled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/mislabeled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\study\\2021_WINTER\\CS175\\CI\\ChatInterpreter\\Utilities.py\u001b[0m in \u001b[0;36mprompt_for_str\u001b[1;34m(message, options)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprompt_for_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# prompt for a string, if there are options, check if within options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[1;34m''' prompt for a string and check if it is in the options, if options not specified, if is returned directly'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    858\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             )\n\u001b[1;32m--> 860\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\conda\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 904\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    905\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
