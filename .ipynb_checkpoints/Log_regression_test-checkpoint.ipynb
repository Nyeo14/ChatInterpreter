{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\wizard king\n",
      "[nltk_data]     rabbit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [82.92]\n",
      "Training AUC value: [84.02]\n",
      " default accuracy: [66.46]\n",
      "\n",
      "Validation/Testing accuracy: [77.87]\n",
      "Validation/Testing AUC value: [75.76]\n",
      " default accuracy: [67.21]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [86.21]\n",
      "Training AUC value: [86.29]\n",
      " default accuracy: [66.26]\n",
      "\n",
      "Validation/Testing accuracy: [80.33]\n",
      "Validation/Testing AUC value: [80.32]\n",
      " default accuracy: [68.03]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [79.63]\n",
      "Training AUC value: [88.02]\n",
      " default accuracy: [65.23]\n",
      "\n",
      "Validation/Testing accuracy: [83.61]\n",
      "Validation/Testing AUC value: [88.97]\n",
      " default accuracy: [72.13]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [84.57]\n",
      "Training AUC value: [87.01]\n",
      " default accuracy: [66.67]\n",
      "\n",
      "Validation/Testing accuracy: [81.15]\n",
      "Validation/Testing AUC value: [84.67]\n",
      " default accuracy: [66.39]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [82.72]\n",
      "Training AUC value: [87.55]\n",
      " default accuracy: [65.02]\n",
      "\n",
      "Validation/Testing accuracy: [83.61]\n",
      "Validation/Testing AUC value: [85.84]\n",
      " default accuracy: [72.95]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [82.30]\n",
      "Training AUC value: [81.37]\n",
      " default accuracy: [66.05]\n",
      "\n",
      "Validation/Testing accuracy: [81.15]\n",
      "Validation/Testing AUC value: [72.21]\n",
      " default accuracy: [68.85]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [79.84]\n",
      "Training AUC value: [85.06]\n",
      " default accuracy: [66.26]\n",
      "\n",
      "Validation/Testing accuracy: [79.51]\n",
      "Validation/Testing AUC value: [88.66]\n",
      " default accuracy: [68.03]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [83.95]\n",
      "Training AUC value: [87.05]\n",
      " default accuracy: [66.67]\n",
      "\n",
      "Validation/Testing accuracy: [88.52]\n",
      "Validation/Testing AUC value: [91.27]\n",
      " default accuracy: [66.39]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [85.19]\n",
      "Training AUC value: [91.95]\n",
      " default accuracy: [67.70]\n",
      "\n",
      "Validation/Testing accuracy: [75.41]\n",
      "Validation/Testing AUC value: [78.18]\n",
      " default accuracy: [62.30]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [84.36]\n",
      "Training AUC value: [85.46]\n",
      " default accuracy: [67.08]\n",
      "\n",
      "Validation/Testing accuracy: [79.51]\n",
      "Validation/Testing AUC value: [78.27]\n",
      " default accuracy: [64.75]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [84.77]\n",
      "Training AUC value: [88.01]\n",
      " default accuracy: [67.90]\n",
      "\n",
      "Validation/Testing accuracy: [77.87]\n",
      "Validation/Testing AUC value: [81.13]\n",
      " default accuracy: [61.48]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [86.42]\n",
      "Training AUC value: [90.38]\n",
      " default accuracy: [65.23]\n",
      "\n",
      "Validation/Testing accuracy: [80.33]\n",
      "Validation/Testing AUC value: [79.18]\n",
      " default accuracy: [72.13]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [84.36]\n",
      "Training AUC value: [84.44]\n",
      " default accuracy: [68.11]\n",
      "\n",
      "Validation/Testing accuracy: [82.79]\n",
      "Validation/Testing AUC value: [86.77]\n",
      " default accuracy: [60.66]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [83.95]\n",
      "Training AUC value: [82.35]\n",
      " default accuracy: [67.49]\n",
      "\n",
      "Validation/Testing accuracy: [74.59]\n",
      "Validation/Testing AUC value: [75.87]\n",
      " default accuracy: [63.11]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [82.10]\n",
      "Training AUC value: [80.99]\n",
      " default accuracy: [67.70]\n",
      "\n",
      "Validation/Testing accuracy: [77.05]\n",
      "Validation/Testing AUC value: [72.80]\n",
      " default accuracy: [62.30]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [83.95]\n",
      "Training AUC value: [84.59]\n",
      " default accuracy: [67.49]\n",
      "\n",
      "Validation/Testing accuracy: [81.15]\n",
      "Validation/Testing AUC value: [80.72]\n",
      " default accuracy: [63.11]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [84.16]\n",
      "Training AUC value: [87.06]\n",
      " default accuracy: [67.28]\n",
      "\n",
      "Validation/Testing accuracy: [79.51]\n",
      "Validation/Testing AUC value: [80.86]\n",
      " default accuracy: [63.93]\n",
      "\n",
      "Number of training examples: [486]\n",
      "Vocabulary size: [2089]\n",
      "Training accuracy: [79.22]\n",
      "Training AUC value: [86.60]\n",
      " default accuracy: [64.81]\n",
      "\n",
      "Validation/Testing accuracy: [76.23]\n",
      "Validation/Testing AUC value: [84.06]\n",
      " default accuracy: [73.77]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "import simplejson as json\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import * \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn import metrics \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from Utilities import *\n",
    "from Tokenizer_kit import *\n",
    "from Embedding import *\n",
    "from Data_loader import *\n",
    "from Data_converter import *\n",
    "from random import shuffle\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# nltk.download('stopwords') is needed\n",
    "# the following 2 functions are from HW1, with some modification.\n",
    "def logistic_classification(X, Y, classifier = None):\n",
    "\tmsg_line = \"\"\n",
    "\tif (classifier == None):\n",
    "\t\tmode = \"Training\"\n",
    "\t\tmsg_line += f\"Number of training examples: [{X.shape[0]}]\" + os.linesep\n",
    "\t\tmsg_line += f\"Vocabulary size: [{X.shape[1]}]\" + os.linesep\n",
    "\t\tclassifier = linear_model.LogisticRegression(penalty = 'l2', tol = 0.3, solver = \"sag\", max_iter = 10)\n",
    "\t\tclassifier.fit(X, Y)\n",
    "\telse:\n",
    "\t\tmode = \"Validation/Testing\"\n",
    "\taccuracy = classifier.score(X, Y)\n",
    "\tmsg_line += mode + f\" accuracy: [{format( 100*accuracy , '.2f')}]\" + os.linesep\n",
    "\ttrain_predictions = classifier.predict(X)\n",
    "\tclass_probabilities = classifier.predict_proba(X)\n",
    "\ttest_auc_score = sklearn.metrics.roc_auc_score(Y, class_probabilities[:,1])\n",
    "\tmsg_line += mode + f\" AUC value: [{format( 100*test_auc_score , '.2f')}]\" + os.linesep\n",
    "\tzero_count = 0\n",
    "\tfor i in Y:\n",
    "\t\tif i == 0:\n",
    "\t\t\tzero_count += 1\n",
    "\tdefault_accuracy = zero_count / len(Y)\n",
    "\tmsg_line += f\" default accuracy: [{format( 100*default_accuracy , '.2f')}]\" + os.linesep\n",
    "\tcounter = 0\n",
    "\tmy_error = []\n",
    "\twhile (counter < X.shape[0]):\n",
    "\t\tif (train_predictions[counter] != Y[counter]):\n",
    "\t\t\tmy_error.append(counter)\n",
    "\t\tcounter += 1\n",
    "\treturn classifier, my_error, msg_line\n",
    "\n",
    "def most_significant_terms(classifier, vectorizer, K):\n",
    "\tcount = 0\n",
    "\ttopK_pos_weights = set()\n",
    "\ttopK_pos_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmax = -1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight > 0 and weight > max and not weight in topK_pos_weights):\n",
    "\t\t\t\tmax = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not max == -1):\n",
    "\t\t\ttopK_pos_weights.add(max)\n",
    "\t\t\ttopK_pos_terms.add(temp_term)\n",
    "\t\t\tprint(\"Positive weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(max))\n",
    "\t\tcount += 1\n",
    "\tcount = 0\n",
    "\ttopK_neg_weights = set()\n",
    "\ttopK_neg_terms = set()\n",
    "\twhile(count < K):\n",
    "\t\tmin = 1\n",
    "\t\ttemp_count = 0\n",
    "\t\ttemp_term = \"null indicator, if the proper word is not found\"\n",
    "\t\tfor weight in classifier.coef_[0]:\n",
    "\t\t\tif (weight < 0 and weight < min and not weight in topK_neg_weights):\n",
    "\t\t\t\tmin = weight\n",
    "\t\t\t\ttemp_term = vectorizer.get_feature_names()[temp_count]\n",
    "\t\t\ttemp_count += 1\n",
    "\t\tif (not min == 1):\n",
    "\t\t\ttopK_neg_weights.add(min)\n",
    "\t\t\ttopK_neg_terms.add(temp_term)\n",
    "\t\t\tprint(\"Negative weight rank \", str(count + 1), \": \")\n",
    "\t\t\tprint(\"--->\", temp_term, \", and its weight is: \", str(min))\n",
    "\t\tcount += 1\n",
    "\treturn(topK_pos_weights, topK_neg_weights, topK_pos_terms, topK_neg_terms)\n",
    "\n",
    "# directly convert a list of long strings into a one-hot vector\n",
    "# it does both tokenization and vectorization\n",
    "# it should returns an 2-D array\n",
    "# X index the clip, Y index the token\n",
    "def to_ohv(text_list, stop_words = [], min_len = 2):\n",
    "\ttoken_set = set()\n",
    "\tfor text in text_list:\n",
    "\t\tfor word in text.split():\n",
    "\t\t\tif (len(word) > min_len) and (not word in stop_words) and (not word in set(stopwords.words('english'))) and (not word in token_set):\n",
    "\t\t\t\ttoken_set.add(word)\n",
    "\tresult = np.zeros((len(text_list), len(token_set)))\n",
    "\tfor i, token in enumerate(token_set):\n",
    "\t\tfor ii, sentence in enumerate(text_list):\n",
    "\t\t\tif (token in sentence):\n",
    "\t\t\t\tresult[ii][i] = 1\n",
    "\treturn result\n",
    "\n",
    "# translate a word into something standard\n",
    "def my_translator(target_word):\n",
    "\tpass\n",
    "\n",
    "# please be sure that you give it a valid path when using it\n",
    "def add_filepath_to_set(the_path:str, is_file:bool, original_set):\n",
    "    if (is_file):\n",
    "        original_set.append(the_path)\n",
    "    else:\n",
    "        for entry in os.scandir(the_path):\n",
    "            if (entry.path.endswith(\".pkl\") and entry.is_file()):\n",
    "                original_set.append(entry.path)\n",
    "    return original_set\n",
    "\n",
    "# interpret a pkl file and extract its data into three lists\n",
    "def add_clipdata_to_set(clip_list, text_list, y_list, pkl_path):\n",
    "    the_file = open(pkl_path, 'rb')\n",
    "    the_pkl = pickle.load(the_file)\n",
    "    for clip in the_pkl:\n",
    "        clip_list.append(clip)\n",
    "        text_list.append(Concatenate_str_list(clip.chats))\n",
    "        if (clip.get_label_binary() == 0):\n",
    "            y_list.append(0)\n",
    "        else:\n",
    "            y_list.append(1)\n",
    "    the_file.close()\n",
    "    return clip_list, text_list, y_list\n",
    "\n",
    "# randomize data\n",
    "def randomize_data(clip_list, text_list, y_list):\n",
    "\torder_list = list(range(len(clip_list)))\n",
    "\trandom.shuffle(order_list)\n",
    "\tnew_clip = list(clip_list)\n",
    "\tnew_text = list(text_list)\n",
    "\tnew_y = list(y_list)\n",
    "\tfor i, ii in enumerate(order_list):\n",
    "\t\tnew_clip[i] = clip_list[ii]\n",
    "\t\tnew_text[i] = text_list[ii]\n",
    "\t\tnew_y[i] = y_list[ii]\n",
    "\treturn new_clip, new_text, new_y\n",
    "\n",
    "# this function iteratively run the main to find the best param\n",
    "def best_param(ngram, panelty, dual, tol, C, fit_intercept, solver, max_iter, num_iter = 10, test_ratio = 0.2, test_on = [\"labeled_clip_data/Teo\", \"labeled_clip_data/wardell\", \"labeled_clip_data/T90\"]):\n",
    "\tva_err_list = []\n",
    "\t#define training set\n",
    "\tfilepath = []\n",
    "\ttext = []\n",
    "\tY = []\n",
    "\tall_clip = []\n",
    "\tfor path in test_on:\n",
    "\t\tfilepath = add_filepath_to_set(path, False, filepath)\n",
    "\tfor filename in filepath:\n",
    "\t\tall_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "\t#define validation set\n",
    "\ttraining_size = int(len(Y) * (1 - test_ratio))\n",
    "\tvalidation_size = len(Y) - training_size\n",
    "\t# iteratively test the model\n",
    "\twhile (num_iter > 0):\n",
    "\t\t# randomize the data\n",
    "\t\tall_clip, text, Y = randomize_data(all_clip, text, Y)\n",
    "\t\t# construct the vectorizer\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, ngram), stop_words = 'english', min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\t\tX = vect.fit_transform(text)\n",
    "\t\t# make classifier\n",
    "\t\t# the following line is responsible for taking different parameters\n",
    "\t\tclassifier = linear_model.LogisticRegression(C = C, dual = dual, penalty = panelty, fit_intercept = fit_intercept, tol = tol, solver = solver, max_iter = max_iter)\n",
    "\t\tclassifier.fit(X[:training_size], Y[:training_size])\n",
    "\t\tva_err_list.append(classifier.score(X[training_size:], Y[training_size:]))\n",
    "\t\tnum_iter -= 1\n",
    "\treturn np.average(va_err_list)\n",
    "\n",
    "def sudo_main(va_ratio = None, folderpath = None, ask_save = True, ask_test = True, if_debug = True, always_default = False): \n",
    "    # main function, a sequence of supportive methods defined above \n",
    "    # see specifications in learner_output.txt \\\n",
    "    # one good practice is to keep indent within a function no more than 3\n",
    "    # if more loop like structures are needed, another defined method is recommended\n",
    "\n",
    "    #define training set\n",
    "    text = []\n",
    "    Y = []\n",
    "    all_clip = []\n",
    "    filepath = []\n",
    "    if (folderpath == None):\n",
    "        filepath = []\n",
    "        file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "        while(file_or_folder != \"e\"):\n",
    "            filepath = add_filepath_to_set(file_or_folder, _type == \"file\", filepath)\n",
    "            file_or_folder, _type = prompt_for_file_folder(\"enter a path to a file or a folder to add that to the training set, enter e to exit\", {\"e\"})\n",
    "    else:\n",
    "        filepath = []\n",
    "        for path in folderpath:\n",
    "            filepath = add_filepath_to_set(path, False, filepath)\n",
    "    for filename in filepath:\n",
    "        all_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, filename)\n",
    "    #define validation set\n",
    "    if va_ratio == None:\n",
    "        validation_ratio = prompt_for_float(\"What proportion of the training data would be used for validation?\", 0, 1)\n",
    "    else:\n",
    "        validation_ratio = va_ratio\n",
    "    training_size = int(len(Y) * (1 - validation_ratio))\n",
    "    validation_size = len(Y) - training_size\n",
    "    # randomize the data\n",
    "    all_clip, text, Y = randomize_data(all_clip, text, Y)\n",
    "    # train the model\n",
    "    classifier, t_err, v_err, t_msg, v_msg = main(text, Y, training_size, validation_size, always_default = always_default)\n",
    "    if if_debug:\n",
    "        print(t_msg)\n",
    "        print(v_msg)\n",
    "    #save the mislabeled\n",
    "    if (ask_save and prompt_for_str(\"Do you want to save the mislabeled clips? (y/n) \") == \"y\"):\n",
    "        if not os.path.isdir(\"/mislabeled\"):\n",
    "            os.mkdir(\"/mislabeled\")\n",
    "        file_prefix = prompt_for_str(\"Please name the prefix of saved files: \")\n",
    "        # making mislabeled file for training errors\n",
    "        err_list = list()\n",
    "        for err_id in t_err:\n",
    "            err_list.append(all_clip[err_id])\n",
    "        new_file_path = 'mislabeled/' + file_prefix + '_mislabeled_train.pkl' \n",
    "        with open(new_file_path, 'wb') as f: \n",
    "            pickle.dump(err_list, f)\n",
    "        # making mislabeled file for validation errors\n",
    "        err_list = list()\n",
    "        for err_id in v_err:\n",
    "            err_list.append(all_clip[err_id + training_size])\n",
    "        new_file_path = 'mislabeled/' + file_prefix + '_mislabeled_validation.pkl' \n",
    "        with open(new_file_path, 'wb') as f: \n",
    "            pickle.dump(err_list, f)\n",
    "    # test the classifier\n",
    "    training_size = len(Y)\n",
    "    while (ask_test and input(\"Do you want to test this classifier on any unlabled clip data? (y/n)\") == \"y\"):\n",
    "        all_clip = []\n",
    "        Y = Y[training_size:]\n",
    "        text = text[training_size:]\n",
    "        file_path = prompt_for_file(\"which file you want to do test on? \")\n",
    "        if_answer = input(\"Is this file labeled? (y/n)\") == \"y\"\n",
    "        all_clip, text, Y = add_clipdata_to_set(all_clip, text, Y, file_path)\n",
    "        classifier, t_err, v_err, t_msg, v_msg = main(text, Y, training_size, len(Y) - training_size, if_answer)\n",
    "        if if_answer:\n",
    "            print(v_msg)\n",
    "        else:\n",
    "            counter = 0\n",
    "            while(counter < len(all_clip)):\n",
    "                all_clip[counter].labeled = v_msg[counter]\n",
    "                counter += 1\n",
    "            file_path = prompt_for_save_file(dir_path='model_labeled_result', f_format='.pkl')\n",
    "            with open(file_path, 'wb') as f: \n",
    "                pickle.dump(all_clip, f)\n",
    "    #return classifier.score(text[:training_size], Y[:training_size]), classifier.score(text[training_size:], Y[training_size:])\n",
    "\n",
    "# main\n",
    "def main(the_text = None, the_y = None, t_size = None, v_size = None, test_has_answer = True, always_default = False):\n",
    "\tif (the_text == None):\n",
    "\t\treturn sudo_main()\n",
    "\t# define stop word\n",
    "\tspecial_stop_word = set(stopwords.words('english'))\n",
    "\tif (not always_default):\n",
    "\t\tif_stop = prompt_for_str(\"Do you want to use default english stopwords or stopwords given by my author? (default/author)\", {\"default\",\"author\"})\n",
    "\t\tif (if_stop == \"author\"):\n",
    "\t\t\tspecial_stop_word = {\"1\", \"2\", \"11\", \"111111\", \"gg\", \"gg gg\", \"LUL\", \"LOL\", \"LUL LUL\", \"T90hype\", \"T90hype T90hype\"}\n",
    "\t# construct the vectorizer\n",
    "\tif (special_stop_word == None):\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, 2), stop_words = 'english', min_df = 0.01, tokenizer = Embedding_tokenize)\n",
    "\telse:\n",
    "\t\tvect = CountVectorizer(ngram_range = (1, 2), stop_words = special_stop_word, min_df = 0.01,  tokenizer = Embedding_tokenize)\n",
    "\tX = vect.fit_transform(the_text)\n",
    "\t#X = to_ohv(the_text)\n",
    "\t# make classifier\n",
    "\tclassifier, t_err, t_msg = logistic_classification(X[:t_size], the_y[:t_size])\n",
    "\tif test_has_answer:\n",
    "\t\t_c, v_err, v_msg = logistic_classification(X[t_size:], the_y[t_size:], classifier)\n",
    "\t# look at result\n",
    "\tif ((not always_default) and input(\"enter y to look at top 5 significant terms, enter other to quit\") == \"y\"):\n",
    "\t\tmost_significant_terms(classifier, vect, 5)\n",
    "\t# return the msg or the labeled clip list\n",
    "\t# whem the validation/test data have answer\n",
    "\tif test_has_answer:\n",
    "\t\treturn classifier, t_err, v_err, t_msg, v_msg\n",
    "\t# when users do not have answer and want to get answer from the model\n",
    "\telse:\n",
    "\t\tv_msg = classifier.predict(X[t_size:])\n",
    "\t\treturn classifier, t_err, \"not valid\", t_msg, v_msg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)\n",
    "    sudo_main(0.2, [\"labeled_clip_data/Teo\",\"labeled_clip_data/T90\",\"labeled_clip_data/wardell\"], False, False, always_default = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8063524590163935\n",
      "0.8081967213114755\n",
      "0.8178278688524591\n",
      "0.8141393442622951\n",
      "0.8155737704918031\n",
      "0.8170081967213114\n",
      "0.8073770491803278\n",
      "0.8206967213114755\n",
      "0.8098360655737704\n",
      "0.812295081967213\n"
     ]
    }
   ],
   "source": [
    "print(best_param(1, 'l2', True, 0.01, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.02, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.08, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.1, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.15, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.2, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.3, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.4, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.5, 1, True, 'liblinear', 50, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8202868852459015\n",
      "0.8198770491803279\n",
      "0.8170081967213114\n",
      "0.8075819672131148\n",
      "0.8161885245901639\n",
      "0.8184426229508196\n",
      "0.8100409836065573\n",
      "0.802049180327869\n",
      "0.8159836065573771\n",
      "0.803483606557377\n"
     ]
    }
   ],
   "source": [
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 10, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 20, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 30, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 40, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 80, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 100, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 150, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 200, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 250, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825\n",
      "0.8159836065573771\n",
      "0.8118852459016394\n",
      "0.8200819672131148\n",
      "0.8155737704918031\n",
      "0.8243852459016393\n",
      "0.8137295081967213\n",
      "0.8075819672131148\n",
      "0.8112704918032787\n",
      "0.8102459016393441\n"
     ]
    }
   ],
   "source": [
    "print(best_param(1, 'l2', True, 0.05, 0.1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 0.2, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 0.3, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 0.4, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 0.5, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 0.6, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 0.7, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 0.8, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 0.9, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8100409836065573\n",
      "0.8256147540983607\n",
      "0.8147540983606556\n",
      "0.8161885245901639\n",
      "0.8204918032786885\n"
     ]
    }
   ],
   "source": [
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', False, 0.05, 1, True, 'sag', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', False, 0.05, 1, True, 'saga', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', False, 0.05, 1, True, 'lbfgs', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', False, 0.05, 1, True, 'newton-cg', 50, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8049180327868853\n",
      "0.8010245901639346\n",
      "0.8036885245901638\n",
      "0.8112704918032787\n"
     ]
    }
   ],
   "source": [
    "print(best_param(1, 'l2', True, 0.05, 1, False, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', False, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', False, 0.05, 1, False, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8092213114754099\n",
      "0.8262295081967214\n",
      "0.8106557377049178\n",
      "0.8170081967213114\n",
      "0.8209016393442624\n",
      "0.8079918032786886\n",
      "0.8116803278688524\n",
      "0.8141393442622951\n"
     ]
    }
   ],
   "source": [
    "print(best_param(1, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(2, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(3, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(4, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(5, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(6, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(7, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))\n",
    "print(best_param(8, 'l2', True, 0.05, 1, True, 'liblinear', 50, 20, 0.4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
